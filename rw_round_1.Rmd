---
title: "Rwanda Soil Health Study - Round 1"
author: '[Matt Lowes](mailto:matt.lowes@oneacrefund.org)'
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  html_notebook:
    number_sections: yes
    code_folding: show
    theme: flatly
    toc: yes
    toc_depth: 6
    toc_float: yes
---

```{r setup, include=FALSE}
#### set up
## clear environment and console
rm(list = ls())
cat("\014")

## set up some global options
# always set stringsAsFactors = F when loading data
options(stringsAsFactors=FALSE)

# show the code
knitr::opts_chunk$set(echo = TRUE)

# define all knitr tables to be html format
options(knitr.table.format = 'html')

# change code chunk default to not show warnings or messages
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

## load libraries
# dplyr and tibble are for working with tables
# reshape is for easy table transformation
# knitr is to make pretty tables at the end
# ggplot2 is for making graphs
# readxl is for reading in Excel files
# MASS is for running boxcox tests
# gridExtra is for arranging plots
# cowplot is for adding subtitles to plots
# robustbase is to run robust regressions to compensate for outliers
# car is for performing logit transformations
libs <- c("dplyr", "reshape2", "knitr", "ggplot2", "tibble", "readxl", 
    "MASS", "gridExtra", "cowplot", "robustbase", "car", "RStata", "foreign",
    "tidyr")
lapply(libs, require, character.only = T, quietly = T, warn.conflicts = F)

#### define helpful functions
# define function to adjust table widths
html_table_width <- function(kable_output, width) {
  width_html <- paste0(paste0('<col width="', width, '">'), collapse = "\n")
  sub("<table>", paste0("<table>\n", width_html), kable_output)
}

options("RStata.StataVersion" = 12)
options("RStata.StataPath" = "/Applications/Stata/StataSE.app/Contents/MacOS/stata-se")
```

```{r}
dataDir <- normalizePath(file.path("..", "..", "data"))
forceUpdateAll <- FALSE
```

# Objectives

The objectives of this notebook are to analyze the results from the first follow up round of the Rwanda long term soil health study.

# Key Takeaways

> Coming soon!

# Data Prep

I'm going to load the baseline data from the baseline analysis. The report and data can be found here. I'll load the new data directly from CommCare. The original baseline data object was `d` but I'm going to make it `b`. Each subsequent round will be `r1`, `r2` and so on.

Overall I want to bring in 3 data sources:

* Basline survey data and soil data
* Round 1 survey and and soil data from 16B
* Round 1 yield and soil data - these data come from paired climbing bean harvest measurements and soil samples from 16B
* We can also look at maize paired yield and soil samples from 17A.

## Baseline data

```{r}
baselineDir <- normalizePath(file.path("..", "rw_baseline", "data"))

load(file=paste0(baselineDir, "/shs rw baseline full soil.Rdata")) # obj d
b <- baseVars
```

**Context point**: The baseline data has `r dim(b)[1]` rows. This is `r 2448-dim(b)[1]` fewer rows than we expected in the baseline. This is because of some farmers not being surveyed as expected. See the baseline report for more details. Also, these baesline values have te

[Alex Villec](matilto:alex.villec@oneacrefund.org) wrote a cleaning script to deal with the first round of Rwanda SHS follow up data and make key adjustments to the data. To utilize that do file here, I'm going to download the data from Commcare, save it, and have the dofile access that file to execute. However, the original file Alex was using had different variable names than the file pulled by the API. The options from here are to just go with the file from Alex or to align the variable names between his version and the CC version. It's valuable to have the data directly from CC but it'll involve more work up front

## Round 1 data

```{r}
source("../oaflib/commcareExport.R")
r <- getFormData("oafrwanda", "M&E", "16B Ubutaka (Soil)", forceUpdate = forceUpdateAll)
write.csv(r, file="rawCcR1Data.csv", row.names = F)
```

## Yield data

```{r}
yp <- getFormData("oafrwanda", "M&E", "16B ALL Isarura (Harvest)", forceUpdate = forceUpdateAll)
write.csv(yp, file="rawCcYpData.csv", row.names=F)
```

The first round of data from CommCare has `r dim(r)[1]` observations. This leaves XX number of farmers unsurveyed in the first survey round. See [this cleaning file](www.github.com) for more information on the farmers we did not find again in the first follow up.

Here I'm going to call the STATA cleaning file to make AV's changes to the R1 follow up data. This requires that the data from CC have the same variable names as the STATA cleaning file. I'm going to try to execute that here:

```{r}
stataDir <- normalizePath(file.path("..", "rw_round_1_check"))
```

Here I access the soil predictions from the OAF soil lab. [Patrick Bell](mailto:patrick.bell@oneacrefund.org) manages the lab and [Mike Barber](mike.barber@oneacrefund.org) oversees the prediction scripts.

```{r}
soilDir <- normalizePath(file.path("..", "..", "OAF Soil Lab Folder", "Projects", "rw_shs_second_round", "4_predicted", "other_summaries"))
soil <- read.csv(file=paste(soilDir, "combined-predictions-including-bad-ones.csv", sep = "/"))
```

## Combine baseline and R1

Combine the available data by farmer and resolve merging issues. These data can be combined long as long as the variable names are consistent or wide. I'm going to combine the data long and use `split` type commands to aggregate the data more easily. Confirm the variable names are consistent. By advancing this code on 5/9/17, I'm for the time being ignoring the cleaning Alex did in his do file. I'll need to go back and incorporate those changes.

**TODO**: see if the variables names in Alex's raw data, shared by [Nathaniel](mailto:nathaniel.rosenblum@oneacrefund.org), match the data I'm downloading from commcare. If so, don't use the `var_names.xlsx` sheet and instead use those variable names and Alex's do file to preserve all of his changes.

Not many of the names are the same. I've downloaded the meta data from CommCare which I'll use to simplify the cleaning of the round 1 data. I'm also going to reshape the baseline variable names to simplify the matching of baseline variables to round 1 variables.
```{r, messages=F}
datNames <- function(dat){
  varNames = names(dat)
  exVal = do.call(rbind, lapply(varNames, function(x){
    val = dat[1:3,x]
    return(val)
  }))
  
  out = cbind(varNames, exVal)
  return(out)
}

baseNames <- datNames(b)
write.csv(baseNames, file="baseline var names.csv", row.names = F)
```

Load Alex's raw data and take the variable names from this. If I can align these variable names with the data from CC I can then execute Alex's cleaning script on the CC data and proceed with combining the data

## Stata .do file

```{r}
rawDir <- normalizePath(file.path("Soil health study (year one)", "data"))

avRaw <- read.csv(paste(rawDir, "y1_shs_rwanda_28sep.csv", sep = "/"), stringsAsFactors = F)

```

It looks like the data from CommCare aligns with the raw data Alex worked with at `info_formid` which is the second index for `avRaw` and the 10th index for `r`. Let's just try transferring them over and the work of updating the variable names through the CC codebook export may not be necessary!

```{r}
varTest <- data.frame(fromcc = names(r)[10:409], fromav = names(avRaw)[2:401])
# head(varTest)
# tail(varTest)
#varTest[90:120,]
```

It seems to line up okay (with some adjustments)! To incorporate Alex's cleaning code I have to export the data from R to a form Stata accept, run the code, and then load the data back in.

This function will remove all strange outputs from the data from CommCare so that the STATA code works

```{r}
charClean <- function(df){
  
  df <- as.data.frame(lapply(df, function(x){
  x = gsub("'", '', x)
  x = gsub("^b", '', x)
  x = ifelse(grepl("map object", x)==T, NA, x)
  return(x)
  }))
return(df)
}

r <- charClean(r)
```

```{r}
names(r)[10:409] <- names(avRaw)[2:401]

#export so stata can run - check for variable names longer than 32char
table(nchar(names(r)))

write.csv(r, file="toBeCleanedStata.csv", row.names = F)

stata("cleans_y1_shs_rwanda.do", stata.echo=F)
```

Now load the result of the Stata file
```{r}
r <- read.csv("cleanedforR.csv", stringsAsFactors = F)
```

```{r,eval=F}
newName <- read_excel("var_names.xlsx", sheet=1)

qTypes <- c("Multiple Choice", "Phone Number or Numeric ID", "Checkbox", "Text", "Decimal", "Image Capture", "Barcode Scan", "GPS", "Integer", "Time", "Date")

newName <- newName %>% dplyr::select(1:6
) %>% dplyr::filter(newName$Type %in% qTypes) %>% as.data.frame()

#newName <- newName %>% filter(new.var.name!="general.comment")
metaVars <- names(r)[1:10]
newNameVars <- c(metaVars, newName$new.var.name)

length(newNameVars)==dim(r)[2]

write.csv(newNameVars, file="newVar check.csv")
write.csv(names(r), file="round1 Var check.csv")

```

```{r, eval=F}
names(r) <- newNameVars

# drop vars with drop
r <- r[,-which(grepl("drop.", names(r)))]
```


```{r, eval=F}
qNum <- c("Phone Number or Numeric ID", "Decimal", "Integer")
nums <- newName[newName$Type %in% qNum, "new.var.name"]
nums <- nums[-which(grepl("drop.", nums))]

toRemove <- c("phone", "oafid")
nums <- nums[!nums %in% toRemove]

# add in plot.size
#nums <- c(nums, "plot.size")

r[, nums] <- as.data.frame(lapply(r[,nums], function(x){
  as.numeric(as.character(x))
}))
```

# Cleaning

The `r` dataframe has many more variables than the baseline survey. This was in part expected; we added questions to the first follow up round based on lessons from the baseline. It's also due to how the survey was set up in CommCare. Before combining the baseline and the first follow up round I need to:

* reshape the round 1 variables so that they appropriately match the baseline variables
* Clean those variales or prepare them as need be for a 
* For variables with no match, clean

```{r}
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

## Drop variables
```{r}
toDrop <- c("appformid", "id", "domain", "metadatadeviceid")
r <- r[,!names(r) %in% toDrop]
```


```{r}
source("../oaflib/misc.R")
names(r) <- gsub("^y1_|intro_", "", names(r))
r[r=="."] <- NA

r <- divideGps(r, "gps_coord")
```

## Categorical variables

The responses of the categorical variables should be regulated through CC, however, to check, make a table that shows the top ten responses in descending order and make a graph of response counts to know what to check. I'll then capture any characters that should be numeric and convert them.

```{r}
catVars <- names(r)[sapply(r, function(x){
  is.character(x)
})]

enumClean <- function(dat, x, toRemove){
  dat[,x] <- ifelse(dat[,x] %in% toRemove, NA, dat[,x])
  return(dat[,x])
}

strTable <- function(dat, x){
  varName = x
  tab = as.data.frame(table(dat[,x], useNA = 'ifany'))
  tab = tab[order(tab$Freq, decreasing = T),]
  end = ifelse(length(tab$Var1)<10, length(tab$Var1), 10)
  repOrder = paste(tab$Var1[1:end], collapse=", ")
  out = data.frame(variable = varName,
                   responses = repOrder)
  
  return(out)
}

# clean up known values
catEnumVals <- c("-99", "-88", "- 99", "-99.0", "88", "_88", "- 88", "0.88",
                 "--88", "__88", "-88.0", "99.0")
r[,catVars] <- sapply(catVars, function(y){
  r[,y] <- enumClean(r,y, catEnumVals)
})


responseTable <- do.call(rbind, lapply(catVars, function(x){
  strTable(r, x)
}))

```

### Categorical response table

A simple table to preview the values in the data. The values are ranked by frequency.

```{r}
kable(responseTable)
```

### Categorical response graphs
```{r}
repGraphs <- function(dat, x){
  tab = as.data.frame(table(dat[,x], useNA = 'ifany'))
  tab = tab[order(tab$Freq, decreasing = T),]
  print(
    ggplot(data=tab, aes(x=Var1, y=Freq)) + geom_bar(stat="identity") +
      theme(legend.position = "bottom", axis.text.x = element_text(angle = 45, hjust = 1)) +
      labs(title =paste0("Composition of variable: ", x))
  )
}

adminVars <- c(names(r)[grep("meta", names(r))], "start_time", "enum_name", "photo", "cell_field", "village", "farmer_respond", "farmer_phonenumber", "d_phone", "neighbor_phonenumber", "farmer_list", "unique_location", "comments", "gps_coord", "sample_id")
nonAdminVars <- catVars[!catVars %in% adminVars]

for(i in 1:length(nonAdminVars)){
  repGraphs(r, nonAdminVars[i])
}
```

### Manual character cleaning
```{r}
r$female <- ifelse(r$gender=="female", 1, 0)
r$district <- ifelse(grepl("nyanza", r$district)==T, "Nyanza", r$district)

table(r$kg_seed_16b_1)
table(r$kg_yield_16a_2)

strtoNum <- c("kg_seed_16b_1", "kg_yield_16a_1", "kg_yield_16b_1", "kg_yield_16b_2")
r[,strtoNum] <- sapply(r[,strtoNum], function(x){as.numeric(x)})
```

Notes on the categorical variables:

* We don't have many actual responses on seed type despite all farmers telling us about a crop they are growing. Why? Check that there wasn't a mislabeling of variables.
* Check the 'which_maize_seed' variables to make certain they're flexible to the type of crop selected in the previous question.
* Confirm that blank is NA not 0.

## Numeric variables

```{r}
numVars <- names(r)[sapply(r, function(x){
  is.numeric(x)
})]
```

Basic cleaning of known issues like enumerator codes for DK, NWR, etc.
```{r}
enumVals <- c(-88,-85, -99)

r[,numVars] <- sapply(numVars, function(y){
  r[,y] <- enumClean(r,y, enumVals)
})
```

### Numeric outlier table

```{r}
iqr.check <- function(dat, x) { 
  q1 = summary(dat[,x])[[2]]
  q3 = summary(dat[,x])[[5]] 
  iqr = q3-q1
  mark  = ifelse(dat[,x] < (q1 - (1.5*iqr)) | dat[,x] > (q3 + (1.5*iqr)), 1,0)
  tab = rbind(
    summary(dat[,x]),
    summary(dat[mark==0, x])
  )
  return(tab)
}

# remove admin vars
numAdminVars <- c(numVars[1:3])
numVarsNotAdmin <- numVars[!numVars %in% numAdminVars]

iqrTab <- do.call(plyr::rbind.fill, lapply(numVarsNotAdmin, function(y){
  #print(y)
  res = iqr.check(r, y)
  #print(dim(res))
  out = data.frame(var=rbind(y, paste(y, ".iqr", sep="")), res)
  return(out)
}))

iqrTab[,2:8] <- sapply(iqrTab[,2:8], function(x){round(x,1)})
```

The outlier table summarizes the numeric variables with and without IQR outliers to show how the data changes based on this filter.

```{r}
knitr::kable(iqrTab, row.names = F, digits = 0, format = 'html')
```

### Outlier Graphs
```{r}
# http://rforpublichealth.blogspot.com/2014/02/ggplot2-cheatsheet-for-visualizing.html
for(i in 1:length(numVarsNotAdmin)){
    base <- ggplot(r, aes(x=r[,numVarsNotAdmin[i]])) + labs(x = numVarsNotAdmin[i])
    temp1 <- base + geom_density()
    temp2 <- base + geom_histogram()
    #temp2 <- boxplot(r[,numVars[i]],main=paste0("Variable: ", numVars[i]))
    multiplot(temp1, temp2, cols = 2)
}
```

## Check for unique ids

I'm seeing that there are duplicated farmers in the data when I'm trying to reshape the `r` data from wide to long. Let's check them out here and see if we can figure out which observation is right. 

* Check Alex's do file to see if there's mention of these farmers. [No mention]
* Check the baseline values as these should line up.

```{r}
length(r$sample_id)==length(unique(r$sample_id))
dups <- r$sample_id[duplicated(r$sample_id)]
dupIndex <- which(duplicated(r$sample_id))

#dupDat <- r[r$sample_id %in% dups,]
head(r[r$sample_id==dups[1],])
head(r[r$sample_id==dups[2],])
```


## Combine long with baseline

Let's first check with the baseline data to see what variables we made there so I can make the same ones from the round 1 data. There are some variables that are baseline variables only like variables asking about historical practices. There are then other variables that will vary by season. These are the variables that we ultimately want in to shape in a long dataset by season to analyze changes overtime in practices and soil management. I think this will result in a dataset that has one row per farmer per season. Some variables may not fit nicely into this but we can deal with those. For variables that aren't changing over time they'll show as not important in our model. They're important for matching farmers.

There are a lot of variables to try to line up. Some already have the same name but how to best combine the ones that have different variable names? I'm going to write a function that takes a variable name from `b` and a variable name from `r` that should go together, updates the `r` variable name and uses that info to `rbind` the data into a long dataset.

```{r}
# names(b)
# names(r)

# check the names that already match
baselineFound <- names(b)[names(b) %in% names(r)] # not many variable names are aligned
```

Update variable names so that any variable with 16a or 16b has a the `a` or `b` season designation at the end it so I can replicate the `gather()` and `spread()` options for reorganizing the data by season and by plot. This means that the variable names will retain their designation of first or second application and be distinguishable.

TODO(mattlowes) - rename the variables according to that convention to reshape the `r` data. Keep the baseline data in mind.

```{r}
r <- r %>% rename(
  which_crop_1_16a = which_crop_16a_1,
  which_maize_seed_1_16a = which_maize_seed_16a_1,
  which_crop_2_16a = which_crop_16a_2,
  which_maize_seed_2_16a = which_maize_seed_16a_2,
  kg_seed_veg_1_16a = kg_seed_veg_16a_1,
  kg_seed_1_16a = kg_seed_16a_1,
  kg_seed_2_16a = kg_seed_16a_2,
  kg_yield_1_16a = kg_yield_16a_1,
  kg_yield_2_16a = kg_yield_16a_2,
  yield_compare_1_16a = yield_compare_16a_1,
  yield_compare_2_16a = yield_compare_16a_2,
  
  which_crop_1_16b = which_crop_16b_1,
  which_maize_seed_1_16b = which_maize_seed_16b_1,
  which_crop_2_16b = which_crop_16b_2,
  which_maize_seed_2_16b = which_maize_seed_16b_2,
  #kg_seed_veg_1_16a = kg_seed_veg_16a_1,
  kg_seed_1_16b = kg_seed_16b_1,
  kg_seed_2_16b = kg_seed_16b_2,
  kg_yield_1_16b = kg_yield_16b_1,
  kg_yield_2_16b = kg_yield_16b_2,
  yield_compare_1_16b = yield_compare_16b_1,
  yield_compare_2_16b = yield_compare_16b_2
)



aSeason <- names(r)[grep("16a", names(r))]
bSeason <- names(r)[grep("16b", names(r))]
seasonalVars <- rbind(aSeason, bSeason, "sample_id")
farmerVars <- names(r)[!names(r) %in% seasonalVars]
```


```{r}
# example data
df <- data.frame(
  id = 1:10,
  time = as.Date('2009-01-01') + 0:9,
  Q3.2.1. = rnorm(10, 0, 1),
  Q3.2.2. = rnorm(10, 0, 1),
  Q3.2.3. = rnorm(10, 0, 1),
  Q3.3.1. = rnorm(10, 0, 1),
  Q3.3.2. = rnorm(10, 0, 1),
  Q3.3.3. = rnorm(10, 0, 1)
)

df %>%
  gather(key, value, -id, -time) %>%
  extract(key, c("question", "loop_number"), "(Q.\\..)\\.(.)") %>%
  spread(question, value)

# aDat <- r[,names(r) %in% aSeason] # works for this too!
# aDat <- aDat[,grep("16a_1", names(aDat))] # works for this
aDat <- r[,names(r) %in% seasonalVars] # works for this!
aDat <- aDat[-dupIndex,]


#http://stackoverflow.com/questions/25925556/gather-multiple-sets-of-columns
# this works with aDat
test <- aDat %>%
  gather(key, value, -sample_id) %>%
  extract(key, c("variable", "season"), "(^.*\\_16)(.)") %>%
  spread(variable, value)


test[76322,]
test[78707,]
test[285:286,]

test <- melt(r, measure.vars = seasonalVars, id.vars = farmerVars)

```

TODO - reshape these variables to have a season variable and then make outcomes for each season.

The `matchRounds` function updates variable names across rounds and reports the index and new name of the variables. I can then take the first part of the list for `dat1` and the second part for `dat2`.

```{r, eval=F}
matchRounds <- function(dat1, dat2, var1, var2, new=NULL, choice="first"){
  
  
  
  if (choice=="first"){
    var2new  = var1
    #names(dat2)[names(dat2)==var2] <- var2new
    return(list(
      list(var1, grep(var1, names(dat1))),
      list(var2new, grep(var2, names(dat2)))
                ))
    
  } else if (choice=="second") {
    var1new = var2
    #names(dat1)[names(dat1)==var1] <- var1new
    return(list(
      list(var1new, grep(var1, names(dat1))),
      list(var2, grep(var2, names(dat2)))
                ))
    
  } else{
    var1new = var2new = new
    #names(dat2)[names(dat2)==var2] <- var2new 
    #names(dat1)[names(dat1)==var1] <- var1new
    return(list(
      list(var1new, grep(var1, names(dat1))),
      list(var2new, grep(var2, names(dat2)))
                ))
  }
} 


namesToUpdate <- rbind(
 c("demographicdate", "date", "first"),
  c("sample", "d_sample", "second")
)


# example
dat1=b
dat2=r
var1 = "field_dim1"
var2 = "field_length"
choice="first"

test <- matchRounds(b, r, "field_dim1", "field_length", choice="first")
test2 <- matchRounds(b, r, "field_dim2", "field_width", choice="first")


test <- lapply(namesToUpdate, function(x,y,z){
  val = matchRounds(b, r, x, y, choice=z)
  return(val)
})

```

## Create new variables

# Analysis

## Demographic summary

## Soil summary

## Longitudinal soil summary

## Matching

## Regressions

# Summary

## Changes to the survey

# Appendix


