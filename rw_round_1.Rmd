---
title: "Rwanda Soil Health Study - Round 1"
author: '[Matt Lowes](mailto:matt.lowes@oneacrefund.org)'
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  html_notebook:
    number_sections: yes
    code_folding: show
    theme: flatly
    toc: yes
    toc_depth: 6
    toc_float: yes
---

```{r setup, include=FALSE}
#### set up
## clear environment and console
rm(list = ls())
cat("\014")

## set up some global options
# always set stringsAsFactors = F when loading data
options(stringsAsFactors=FALSE)

# show the code
knitr::opts_chunk$set(echo = TRUE)

# define all knitr tables to be html format
options(knitr.table.format = 'html')

# change code chunk default to not show warnings or messages
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

## load libraries
# dplyr and tibble are for working with tables
# reshape is for easy table transformation
# knitr is to make pretty tables at the end
# ggplot2 is for making graphs
# readxl is for reading in Excel files
# MASS is for running boxcox tests
# gridExtra is for arranging plots
# cowplot is for adding subtitles to plots
# robustbase is to run robust regressions to compensate for outliers
# car is for performing logit transformations
libs <- c("dplyr", "reshape2", "knitr", "ggplot2", "tibble", "readxl", 
    "MASS", "gridExtra", "cowplot", "robustbase", "car", "RStata", "foreign",
    "tidyr", "readxl")
lapply(libs, require, character.only = T, quietly = T, warn.conflicts = F)

#### define helpful functions
# define function to adjust table widths
html_table_width <- function(kable_output, width) {
  width_html <- paste0(paste0('<col width="', width, '">'), collapse = "\n")
  sub("<table>", paste0("<table>\n", width_html), kable_output)
}

options("RStata.StataVersion" = 12)
options("RStata.StataPath" = "/Applications/Stata/StataSE.app/Contents/MacOS/stata-se")
```

# Objectives

The objectives of this notebook are to analyze the results from the first follow up round of the Rwanda long term soil health study.

# Key Takeaways

> See section with [Notes for Nathaniel](#lessons-for-nathaniel)

> See section with [Notes for Patrick and Step](#soil-notes-for-patrick-and-step)

> [Paired Yield and Soil](#clean-soil-ids) ids are a mess. We lose a lot of observations due to unreconciliable duplicates or ids that simply don't have a match. We lose almost 500 observations.

> See [initial yield response analysis](#individual-soil-models)

TODO - check projection from baseline maps, are they shifted over?
TODO - how to connect photos to farmers for enumerators

# Data Prep

I'm going to load the baseline data from the baseline analysis. The report and data can be found here. I'll load the new data directly from CommCare. The original baseline data object was `d` but I'm going to make it `b`. Each subsequent round will be `r1`, `r2` and so on.

Overall I want to bring in 3 data sources:

* Basline survey data and soil data
* Round 1 survey and and soil data from 16B
* Round 1 yield and soil data - these data come from paired climbing bean harvest measurements and soil samples from 16B
* We can also look at maize paired yield and soil samples from 17A.

## Baseline data

```{r}
dataDir <- normalizePath(file.path("..", "..", "data"))
forceUpdateAll <- FALSE
```

```{r}
baselineDir <- normalizePath(file.path("..", "rw_baseline", "data"))

load(file=paste0(baselineDir, "/shs rw baseline full soil.Rdata")) # obj d
b <- baseVars
```

**Context point**: The baseline data has `r dim(b)[1]` rows. This is `r 2448-dim(b)[1]` fewer rows than we expected in the baseline. This is because of some farmers not being surveyed as expected. See the baseline report for more details. Also, these baesline values have te

[Alex Villec](matilto:alex.villec@oneacrefund.org) wrote a cleaning script to deal with the first round of Rwanda SHS follow up data and make key adjustments to the data. To utilize that do file here, I'm going to download the data from Commcare, save it, and have the dofile access that file to execute. However, the original file Alex was using had different variable names than the file pulled by the API. The options from here are to just go with the file from Alex or to align the variable names between his version and the CC version. It's valuable to have the data directly from CC but it'll involve more work up front

## Round 1 data

```{r}
source("../oaflib/commcareExport.R")
r <- getFormData("oafrwanda", "M&E", "16B Ubutaka (Soil)", forceUpdate = forceUpdateAll)
write.csv(r, file="rawCcR1Data.csv", row.names = F)
```

The first round of data from CommCare has `r dim(r)[1]` observations. This leaves XX number of farmers unsurveyed in the first survey round. See [this cleaning file](www.github.com) for more information on the farmers we did not find again in the first follow up.

Here I'm going to call the STATA cleaning file to make AV's changes to the R1 follow up data. This requires that the data from CC have the same variable names as the STATA cleaning file. I'm going to try to execute that here:

```{r}
stataDir <- normalizePath(file.path("..", "rw_round_1_check"))
```

Here I access the soil predictions from the OAF soil lab. [Patrick Bell](mailto:patrick.bell@oneacrefund.org) manages the lab and [Mike Barber](mike.barber@oneacrefund.org) oversees the prediction scripts.

```{r}
soilDir <- normalizePath(file.path("..", "..", "OAF Soil Lab Folder", "Projects", "rw_shs_second_round", "4_predicted", "other_summaries"))
soil <- read.csv(file=paste(soilDir, "combined-predictions-including-bad-ones.csv", sep = "/"))

idDir <- normalizePath(file.path("..", "..", "OAF Soil Lab Folder", "Projects", "rw_shs_second_round", "5_merged"))
Identifiers <- read_excel(paste(idDir,"database.xlsx",sep="/"), sheet=1)
```

Combine the available data by farmer and resolve merging issues. These data can be combined long as long as the variable names are consistent or wide. I'm going to combine the data long and use `split` type commands to aggregate the data more easily. Confirm the variable names are consistent. By advancing this code on 5/9/17, I'm for the time being ignoring the cleaning Alex did in his do file. I'll need to go back and incorporate those changes.

**TODO**: see if the variables names in Alex's raw data, shared by [Nathaniel](mailto:nathaniel.rosenblum@oneacrefund.org), match the data I'm downloading from commcare. If so, don't use the `var_names.xlsx` sheet and instead use those variable names and Alex's do file to preserve all of his changes.

Not many of the names are the same. I've downloaded the meta data from CommCare which I'll use to simplify the cleaning of the round 1 data. I'm also going to reshape the baseline variable names to simplify the matching of baseline variables to round 1 variables.
```{r, messages=F}
datNames <- function(dat){
  varNames = names(dat)
  exVal = do.call(rbind, lapply(varNames, function(x){
    val = dat[1:3,x]
    return(val)
  }))
  
  out = cbind(varNames, exVal)
  return(out)
}

baseNames <- datNames(b)
write.csv(baseNames, file="baseline var names.csv", row.names = F)
```

Load Alex's raw data and take the variable names from this. If I can align these variable names with the data from CC I can then execute Alex's cleaning script on the CC data and proceed with combining the data

## Stata .do file

```{r}
rawDir <- normalizePath(file.path("Soil health study (year one)", "data"))

avRaw <- read.csv(paste(rawDir, "y1_shs_rwanda_28sep.csv", sep = "/"), stringsAsFactors = F)

```

It looks like the data from CommCare aligns with the raw data Alex worked with at `info_formid` which is the second index for `avRaw` and the 10th index for `r`. Let's just try transferring them over and the work of updating the variable names through the CC codebook export may not be necessary!

```{r}
varTest <- data.frame(fromcc = names(r)[10:409], fromav = names(avRaw)[2:401])
# head(varTest)
# tail(varTest)
#varTest[90:120,]
write.csv(varTest, file="variableNameCheck.csv")
```

It seems to line up okay (with some adjustments)! To incorporate Alex's cleaning code I have to export the data from R to a form Stata accept, run the code, and then load the data back in.

This function will remove all strange outputs from the data from CommCare so that the STATA code works

```{r}
charClean <- function(df){
  
  df <- as.data.frame(lapply(df, function(x){
  x = gsub("'", '', x)
  x = gsub("^b", '', x)
  x = ifelse(grepl("map object", x)==T, NA, x)
  return(x)
  }))
return(df)
}

r <- charClean(r)
```

Here is where I actually update the names in `r` to match Alex's original data.

```{r}
names(r)[10:409] <- names(avRaw)[2:401]

#export so stata can run - check for variable names longer than 32char
table(nchar(names(r)))

write.csv(r, file="toBeCleanedStata.csv", row.names = F)

stata("cleans_y1_shs_rwanda.do", stata.echo=F)
```

Now load the result of the Stata file
```{r}
r <- read.csv("cleanedforR.csv", stringsAsFactors = F)
```


# Cleaning

The `r` dataframe has many more variables than the baseline survey. This was in part expected; we added questions to the first follow up round based on lessons from the baseline. It's also due to how the survey was set up in CommCare. Before combining the baseline and the first follow up round I need to:

* reshape the round 1 variables so that they appropriately match the baseline variables
* Clean those variales or prepare them as need be for a 
* For variables with no match, clean

```{r}
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

## Drop variables
```{r}
toDrop <- c("appformid", "id", "domain", "metadatadeviceid")
r <- r[,!names(r) %in% toDrop]
```


```{r}
source("../oaflib/misc.R")
names(r) <- gsub("^y1_|intro_", "", names(r))
r[r=="."] <- NA

r <- divideGps(r, "gps_coord")
```

## Categorical variables

The responses of the categorical variables should be regulated through CC, however, to check, make a table that shows the top ten responses in descending order and make a graph of response counts to know what to check. I'll then capture any characters that should be numeric and convert them.

```{r}
catVars <- names(r)[sapply(r, function(x){
  is.character(x)
})]

enumClean <- function(dat, x, toRemove){
  dat[,x] <- ifelse(dat[,x] %in% toRemove, NA, dat[,x])
  return(dat[,x])
}

strTable <- function(dat, x){
  varName = x
  tab = as.data.frame(table(dat[,x], useNA = 'ifany'))
  tab = tab[order(tab$Freq, decreasing = T),]
  end = ifelse(length(tab$Var1)<10, length(tab$Var1), 10)
  repOrder = paste(tab$Var1[1:end], collapse=", ")
  out = data.frame(variable = varName,
                   responses = repOrder)
  
  return(out)
}

# clean up known values
catEnumVals <- c("-99", "-88", "- 99", "-99.0", "88", "_88", "- 88", "0.88",
                 "--88", "__88", "-88.0", "99.0")
r[,catVars] <- sapply(catVars, function(y){
  r[,y] <- enumClean(r,y, catEnumVals)
})


responseTable <- do.call(rbind, lapply(catVars, function(x){
  strTable(r, x)
}))

```

### Categorical response table

A simple table to preview the values in the data. The values are ranked by frequency.

```{r}
kable(responseTable)
```

### Categorical response graphs
```{r}
repGraphs <- function(dat, x){
  tab = as.data.frame(table(dat[,x], useNA = 'ifany'))
  tab = tab[order(tab$Freq, decreasing = T),]
  print(
    ggplot(data=tab, aes(x=Var1, y=Freq)) + geom_bar(stat="identity") +
      theme(legend.position = "bottom", axis.text.x = element_text(angle = 45, hjust = 1)) +
      labs(title =paste0("Composition of variable: ", x))
  )
}

adminVars <- c(names(r)[grep("meta", names(r))], "start_time", "enum_name", "photo", "cell_field", "village", "farmer_respond", "farmer_phonenumber", "d_phone", "neighbor_phonenumber", "farmer_list", "unique_location", "comments", "gps_coord", "sample_id", "SSN")
nonAdminVars <- catVars[!catVars %in% adminVars]

for(i in 1:length(nonAdminVars)){
  repGraphs(r, nonAdminVars[i])
}
```

### Manual character cleaning
```{r}
r$female <- ifelse(r$gender=="female", 1, 0)
r$district <- ifelse(grepl("nyanza", r$district)==T, "Nyanza", r$district)

#table(r$kg_seed_16b_1)
#table(r$kg_yield_16a_2)

strtoNum <- c("kg_seed_16b_1", "kg_yield_16a_1", "kg_yield_16b_1", "kg_yield_16b_2")
r[,strtoNum] <- sapply(r[,strtoNum], function(x){as.numeric(x)})
```

Notes on the categorical variables:

* We don't have many actual responses on seed type despite all farmers telling us about a crop they are growing. Why? Check that there wasn't a mislabeling of variables.
* Check the 'which_maize_seed' variables to make certain they're flexible to the type of crop selected in the previous question.
* Confirm that blank is NA not 0.

## Numeric variables

```{r}
numVars <- names(r)[sapply(r, function(x){
  is.numeric(x)
})]
```

Basic cleaning of known issues like enumerator codes for DK, NWR, etc.
```{r}
enumVals <- c(-88,-85, -99)

r[,numVars] <- sapply(numVars, function(y){
  r[,y] <- enumClean(r,y, enumVals)
})
```

### Numeric outlier table

```{r}
iqr.check <- function(dat, x) { 
  q1 = summary(dat[,x])[[2]]
  q3 = summary(dat[,x])[[5]] 
  iqr = q3-q1
  mark  = ifelse(dat[,x] < (q1 - (1.5*iqr)) | dat[,x] > (q3 + (1.5*iqr)), 1,0)
  tab = rbind(
    summary(dat[,x]),
    summary(dat[mark==0, x])
  )
  return(tab)
}

# remove admin vars
numAdminVars <- c(numVars[1:3])
numVarsNotAdmin <- numVars[!numVars %in% numAdminVars]

iqrTab <- do.call(plyr::rbind.fill, lapply(numVarsNotAdmin, function(y){
  #print(y)
  res = iqr.check(r, y)
  #print(dim(res))
  out = data.frame(var=rbind(y, paste(y, ".iqr", sep="")), res)
  return(out)
}))

iqrTab[,2:8] <- sapply(iqrTab[,2:8], function(x){round(x,1)})
```

The outlier table summarizes the numeric variables with and without IQR outliers to show how the data changes based on this filter.

```{r}
knitr::kable(iqrTab, row.names = F, digits = 0, format = 'html')
```

### Outlier Graphs
```{r}
# http://rforpublichealth.blogspot.com/2014/02/ggplot2-cheatsheet-for-visualizing.html
for(i in 1:length(numVarsNotAdmin)){
    base <- ggplot(r, aes(x=r[,numVarsNotAdmin[i]])) + labs(x = numVarsNotAdmin[i])
    temp1 <- base + geom_density()
    temp2 <- base + geom_histogram()
    #temp2 <- boxplot(r[,numVars[i]],main=paste0("Variable: ", numVars[i]))
    multiplot(temp1, temp2, cols = 2)
}
```

## Clean soil values

Here is where I will clean soil values before merging them in.

## Merge in soil data

First merge the soil data with the identifiers as we should get full matches. Then merge soil data to the survey data

```{r}
Identifiers <- Identifiers %>% rename(
  sample_id = `Sample ID`,
  SSN = `Lab ssn`
) %>% mutate(
  sample_id = gsub(" ", "", tolower(sample_id))
)

table(Identifiers$SSN %in% soil$SSN) # full matches

soil <- left_join(soil, Identifiers[, c("SSN", "sample_id")], by="SSN") 
```

We have some surveys that don't have soil data. It seems the soil sample id in the `Identifiers` data are a bit messy. Let's clean both up above by removing spaces and making lower case.

```{r}
r$sample_id <- tolower(r$sample_id)

table(r$sample_id %in% soil$sample_id)
r$sample_id[!r$sample_id %in% soil$sample_id]

write.csv(r$sample_id[!r$sample_id %in% soil$sample_id], "surveysWoSoil.csv", row.names = F)
```

And some soil sample_id that don't have a survey
```{r}
soil$sample_id[!soil$sample_id %in% r$sample_id]
write.csv(soil$sample_id[!soil$sample_id %in% r$sample_id], "soilsWoSurvey.csv", row.names = F)
```

```{r}
dim(r)
r <- left_join(r, soil, by="sample_id")
dim(r) # why is it one row longer after the left_join?
```


## Soil values
```{r}
ggplot(r, aes(x=Calcium, y=Magnesium)) + geom_point() +
    stat_smooth(method="loess") +
    labs(x = "Calcium (m3)", y= "Magnesium (m3)", title="Calcium and Magnesium relationship")

ggplot(r, aes(x=pH, y=Calcium)) + geom_point() +
  stat_smooth(method="loess") +
  labs(x = "pH", y="Calcium (m3)", title = "pH and Calcium relationship")

ggplot(r, aes(x=pH, y=Magnesium)) + geom_point() +
  stat_smooth(method="loess") +
  labs(x = "pH", y="Magnesium (m3)", title = "pH and Magnesium relationship")

ggplot(r, aes(x=pH, y=X.Exchangeable.Acidity)) + geom_point() +
  stat_smooth(method="loess") +
  labs(x = "pH", y="Exchangeable Aluminum", title = "pH and Aluminum relationship")

ggplot(r, aes(x=X.Organic.Carbon, y=X.Total.Nitrogen)) + geom_point() + 
  stat_smooth(method="loess") +
  labs(x = "Total Carbon", y="Total Nitrogen", title = "Carbon and Nitrogen relationship")
```

```{r}
soilVars <- names(r)[which(names(r)=="pH"):which(names(r)=="X.Total.Nitrogen")]
```

### Initial T vs. C soil comparison

**Please note**: These are raw comparisons and thus should not be taken as initial findings for how T and C farmers compare. Farmers will be matched to ensure a proper comparison.

```{r}
for(i in 1:length(soilVars)){
  p1 <- ggplot(data=r, aes(x=as.factor(d_client_16b), y=r[,soilVars[i]])) + 
    geom_boxplot() +
    labs(x="Tubura Farmer", y=soilVars[i])
  p2 <- ggplot(data=r, aes(x=r[,soilVars[i]])) + 
    geom_density() + 
    labs(x=soilVars[i])
  multiplot(p1, p2, cols=2)
}


```

### Soil notes for Patrick and Step

* The carbon vs. nitrogen scatter plot looks odd in that the values are clumped in discrete lines. Why might that be?
* What are appropriate cutoff values for the lab predictions? (Patrick, as a general question, we should probably apply those cutoffs to any lab data before sharing it with the teams to simplify working with those data)

## Check for unique ids

I'm seeing that there are duplicated farmers in the data when I'm trying to reshape the `r` data from wide to long. Let's check them out here and see if we can figure out which observation is right. 

* Check Alex's do file to see if there's mention of these farmers. [No mention]
* Check the baseline values as these should line up.

```{r}
length(r$sample_id)==length(unique(r$sample_id))
dups <- r$sample_id[duplicated(r$sample_id)]
dupIndex <- which(duplicated(r$sample_id))

#dupDat <- r[r$sample_id %in% dups,]
#head(r[r$sample_id==dups[1],])
#head(r[r$sample_id==dups[2],])
```

Let's solve the unique id issue by looking at identifying information in the baseline data
```{r}

roundId <- r %>%
  dplyr::select(district, cell_field, village, sample_id, farmer_list) %>%
  filter(r$sample_id %in% dups)



#d
load("rawBaselineWithIdentifers.Rdata")
baseId <- d %>% 
  dplyr::select(district, selected_cell, umudugudu,  sample_id, farmer_name ) %>%
  filter(d$sample_id %in% dups)

#baseId
#roundId

```

### Correct duplicates

Correct the duplicates I can and drop the others for now. Flag the duplicated ones and save them to share with Nathaniel.

TODO(mattlowes) - share any remaining duplicates with Nathaniel and see if he has a solution. Also see if he can understand why this might have happened and if they should actually have a different sample id.

* share the merged data for Nathaniel to put into CC (include the duplicate ids)

```{r}
r <- r %>% mutate(
    dup = ifelse(
      sample_id == "12" & cell_field == "MUNANIRA" |
      sample_id == "137" & village == "Rusuma" |
      sample_id == "1503" & farmer_list=="NAKAGIZE Val\\xc3\\xa9rie" |
      #sample_id == "2044C" &  # same!
      sample_id == "2278" & cell_field=="Nkira A" | # check this as maybe this was the only thing wrong?
      #sample_id == "2299" & # same!
      sample_id == "2610" & village=="agakiri" #|  #agakiri is close to gakiri in spelling. Is this just a typo?
      #sample_id == "2612" &  # same names!
      #sample_id == "2612C" # same names!
      , 1, 0)
) %>% filter(
  dup!=1
) %>% dplyr::select(-dup) 

# run this code again from above to get updated duplicates list
#length(r$sample_id)==length(unique(r$sample_id))
dups <- r$sample_id[duplicated(r$sample_id)]
dupIndex <- which(duplicated(r$sample_id))

# for the time being drop the observations that are duplicates
r <- r[!r$sample_id %in% dups,]

```

## Reshape variables

This should include the baseline variables as well.

Let's first check with the baseline data to see what variables we made there so I can make the same ones from the round 1 data. There are some variables that are baseline variables only like variables asking about historical practices. There are then other variables that will vary by season. These are the variables that we ultimately want in to shape in a long dataset by season to analyze changes overtime in practices and soil management. I think this will result in a dataset that has one row per farmer per season. Some variables may not fit nicely into this but we can deal with those. For variables that aren't changing over time they'll show as not important in our model. They're important for matching farmers.

There are a lot of variables to try to line up. Some already have the same name but how to best combine the ones that have different variable names? I'm going to write a function that takes a variable name from `b` and a variable name from `r` that should go together, updates the `r` variable name and uses that info to `rbind` the data into a long dataset.

```{r}
# names(b)
# names(r)

# check the names that already match
baselineFound <- names(b)[names(b) %in% names(r)] # not many variable names are aligned
```

Update variable names so that any variable with 16a or 16b has a the `a` or `b` season designation at the end it so I can replicate the `gather()` and `spread()` options for reorganizing the data by season and by plot. This means that the variable names will retain their designation of first or second application and be distinguishable.

TODO(mattlowes) - rename the variables according to that convention to reshape the `r` data. Keep the baseline data in mind as we'll want to do the same thing with the baseline data to make them match.

```{r}
r <- r %>% rename(
  which_crop_1_16a = which_crop_16a_1,
  which_maize_seed_1_16a = which_maize_seed_16a_1,
  which_crop_2_16a = which_crop_16a_2,
  which_maize_seed_2_16a = which_maize_seed_16a_2,
  kg_seed_veg_1_16a = kg_seed_veg_16a_1,
  kg_seed_1_16a = kg_seed_16a_1,
  kg_seed_2_16a = kg_seed_16a_2,
  kg_yield_1_16a = kg_yield_16a_1,
  kg_yield_2_16a = kg_yield_16a_2,
  yield_compare_1_16a = yield_compare_16a_1,
  yield_compare_2_16a = yield_compare_16a_2,
  
  which_crop_1_16b = which_crop_16b_1,
  which_maize_seed_1_16b = which_maize_seed_16b_1,
  which_crop_2_16b = which_crop_16b_2,
  which_maize_seed_2_16b = which_maize_seed_16b_2,
  #kg_seed_veg_1_16a = kg_seed_veg_16a_1,
  #kg_seed_ananas_2_16a = kg_seed_ananas_16a_2,
  #kg_seed_hwag_1_16a = kg_seed_hwag_16a_1,
  kg_seed_1_16b = kg_seed_16b_1,
  kg_seed_2_16b = kg_seed_16b_2,
  kg_yield_1_16b = kg_yield_16b_1,
  kg_yield_2_16b = kg_yield_16b_2,
  yield_compare_1_16b = yield_compare_16b_1,
  yield_compare_2_16b = yield_compare_16b_2
)



aSeason <- names(r)[grep("(1.a)", names(r))]
bSeason <- names(r)[grep("(1.b)", names(r))]
seasonalVars <- c(aSeason, bSeason, "sample_id")
farmerVars <- c(names(r)[!names(r) %in% seasonalVars], "sample_id")
```

```{r}
# example data
# df <- data.frame(
#   id = 1:10,
#   time = as.Date('2009-01-01') + 0:9,
#   Q3.2.1. = rnorm(10, 0, 1),
#   Q3.2.2. = rnorm(10, 0, 1),
#   Q3.2.3. = rnorm(10, 0, 1),
#   Q3.3.1. = rnorm(10, 0, 1),
#   Q3.3.2. = rnorm(10, 0, 1),
#   Q3.3.3. = rnorm(10, 0, 1)
# )
# 
# df %>%
#   gather(key, value, -id, -time) %>%
#   extract(key, c("question", "loop_number"), "(Q.\\..)\\.(.)") %>%
#   spread(question, value)
```

```{r}
source("../oaflib/misc.R")
# aDat <- r[,names(r) %in% aSeason] # works for this too!
# aDat <- aDat[,grep("16a_1", names(aDat))] # works for this
aDat <- r[,names(r) %in% seasonalVars] # works for this!

#http://stackoverflow.com/questions/25925556/gather-multiple-sets-of-columns
seasonalDat <- aDat %>%
  gather(key, value, -sample_id) %>%
  tidyr::extract(key, c("variable", "season"), "(^.*\\_1.)(.)") %>%
  mutate(season = paste0("16", season)) %>% 
  spread(variable, value)

names(seasonalDat) <- gsub("_16", "", names(seasonalDat))

```

TODO(mattlowes) - confirm that the tidyr process worked as I expected as there are numerous missing values. These seem to appear where the variable only had one version of the variable, _16, rather than a _16a and a _16b. Check out how this is handling variables with _17 instead of _16.

## Merge seasonal and demographic data

```{r}
rs <- left_join(seasonalDat, r[,c(names(r)[!names(r) %in% seasonalVars],"sample_id")], by="sample_id")
```

## Combine long with baseline

The `matchRounds` function updates variable names across rounds and reports the index and new name of the variables. I can then take the first part of the list for `dat1` and the second part for `dat2`.

Or just change baseline variable names manually. What's the best way to do this? First reshape the baseline variables to be plot level as well with a season indicator. 

TODO(matt.lowes) Confirm that this is necessary. If the baseline data only includes the previous season and the history then the reshape may not be necessary. All subsequent surveys asked about two seasons, the intervening season and the relevant season. Get your head around the baseline data again and act.

```{r}
# b <- b %>% rename(
#   inputuse_priord_fertilizer_15b = inputuse_15b_priord_fertilizer,
#   inputuse_priorculture_15b_1 = inputuse_15b_priorculture_15b_1,
#   inputuse_priord_intercrop_15b = inputuse_15b_priord_intercrop_15b,
#   inputuse_priorculture_in_15b = inputuse_15b_priorculture_15b_in,
#   crop1_seety_15b = crop1_15b_seedty,
#   #v58
#   crop1_yield_15b = crop1_15b_yield,
#   crop1_yield__15b = crop1_15b_yield_,
#   crop2_seedty_15b = crop2_15b_seedty,
#   #63
#   crop2_seedkg_15b = crop2_15b_seedkg,
#   crop2_yield_16b = crop2_15b_yield,
#   crop2_yield__15b = crop2_15b_yield_,
#   field_fert_t_15b = field_15b_fert_t,
#   #v69
#   field_compost_qu_15b = field_compost_qu
# )

```

I think that all needs to be done is to add a season variable and rename the baseline variables to take off the `_15b` portion.

```{r}
write.csv(names(b), "baselineVars.csv", row.names = T)
write.csv(names(rs), "round1Vars.csv", row.names = T)

names(b) <- gsub("_15b", "", names(b))
b$season <- "15b"

b <- b %>% rename(
      crop1_local = v58,
      crop2_local = v63,
      field_fert_t_1 = field_fert_t,
      field_fert_t_2 = v69
    )
```


TODO - it also seems to the case that some of the seed type variables are mixed up in `r` and `rs`. See what the issue is. Each plot should have only one answer for those.

MAJOR TODO: confirm that I'm not duplicating the soil data by assigning it to both of the seasons we asked about in the follow up survey (I think I currently am 6/15/17). We want to account for field management in the intervening season but **we don't want to assume the soil outcome is the same for both seasons. Specifically, this means the 16a season**

**Note**: the final long data by plot should have only one observation for stationary variables like slope or historical information

```{r}
# i'm updating baseline names to match round 1 names. 
bUpdate <- b %>% 
  mutate(
    d_compost = ifelse(field_kg_compost > 0, 1, 0)
  ) %>%
  rename(
  tablet = demographicid_tablet,
  village = umudugudu,
  n_household = hhsize,
  n_tubura_season = total.seasons,
  field_length = field_dim1, # I'm assuming dim1 is length. it might not be. It might not matter.
  field_width = field_dim2,
  n_spots = n_spots_c,
  kg_seed_1 = crop1_seedkg,
  kg_seed_2 = crop2_seedkg,
  fert_kg1 = field_kg_fert1,
  fert_kg2 = field_kg_fert2,
  kg_yield_1 = crop1_yield,
  kg_yield_2 = crop2_yield,
  kg_compost = field_kg_compost,
  d_client = client,
  cell_field = cellule_field,
  fert_type1 = field_fert_t_1,
  fert_type2 = field_fert_t_2,
  X.Total.Nitrogen = Total.Nitrogen,
  X.Sodium = Sodium,
  X.Organic.Carbon = Organic.Carbon,
  X.EC..Salts. = EC..Salts.,
  X.C.E.C = C.E.C,
  X.Exchangeable.Acidity = Exchangeable.Acidity,
  X.Exchangeable.Aluminium = Exchangeable.Aluminium,
  X.Phosphorus.Sorption.Index..PSI. = Acid.Saturation, # check that this is right
  n_cows = betail_ownedn_inka,
  n_goats = betail_ownedn_ihene,
  n_chickens = betail_ownedn_inkoko,
  n_pigs = betail_ownedn_ingurube,
  n_sheep = betail_ownedn_intama,
  date = demographicdate,
  field_slope = general_field_infograde_hill,
  field_erosion = general_field_infoantierosion_ef,
  type_compost = field_type_compo,
  quality_compost = field_compost_qu,
  d_sample = sample,
  enum_name = surveyor,
  how_use_residues = action_cropresid
)

# biographical variales that apply to actions in the baseline before the study started
bioVars <- bUpdate %>% dplyr::select(
  n_season_fert, nofert_why, n_season_compost, nocompost_why, n_season_lime, nolime_why,
  n_season_fallow, n_seasons_leg_1, n_seasons_leg_2, aez, contains("d_season_listd_"),
  contains("inputuse_prior")
)

bVars <- names(bUpdate)[!names(bUpdate) %in% names(bioVars)] # remove biographical vars

# organizational variables to be ignored
orgVars <- bUpdate %>%
  dplyr::select(
    fieldcollectiondate, datecollectedindistrict, datesenttohq, datereceivedathq,
    processedathq_, packedforsendingtokenya_, datefinishedprocessing
  )

bVars <- bVars[!bVars %in% names(orgVars)]

# variables that only appear in the round 1 data >> likely want to keep these and make them part of the "stable" identifying data
onlyR1 <- rs %>%
  dplyr::select(
    field_n_crops, crop_direction, field_texture
  )

r1Vars <- names(rs)[!names(rs) %in% names(onlyR1)]

# check what's already the same
matchNames <- r1Vars[r1Vars %in% bVars] # these are the matches we're getting
# matchNames

# check what isn't accounted for somehow
unmatchedB <- bVars[!bVars %in% r1Vars] # unmatched baseline minus demographic vars
unmatchedRs <- r1Vars[!r1Vars %in% bVars] # unmatched r1
```

Make the sample id lower case

```{r}
bUpdate$sample_id <- tolower(bUpdate$sample_id)
rs$sample_id <- tolower(rs$sample_id)
```


## Merge demographic variables

* Identify demographic and historical variables in `b`
* Identify any new data from R1 not in the baseline and merge them in
* I'm using `bUpdate` as it's the most up to date and simplifies updating the script.

```{r}
bDemo <- bUpdate %>% 
  dplyr::select(
  SSN, district, cell_field, village, sample_id,  
  n_season_fert, nofert_why, n_season_compost, nocompost_why, n_season_lime, nolime_why,
  n_season_fallow, n_seasons_leg_1, n_seasons_leg_2, aez, contains("d_season_listd_"),
  contains("inputuse_prior")
)
```

## Append field level variables

* `rbind` R1 field level variables with `b` field level variables to make a plot level dataset. 
* Select only the variables I want to keep
* Generate any new outcomes that bring the data down to a single outcome, rather than one by plot **and season**.
* I can then make longitudinal outcomes from those data and merge those into the demographic data

```{r}
commonVars <- names(rs)[names(rs) %in% names(bUpdate)]

fieldDat <- rbind(bUpdate[,commonVars], rs[,commonVars])

soilDat <- fieldDat %>% 
  dplyr::select(one_of(soilVars), SSN, season, sample_id, d_client) %>%
  filter(season!="16a") # dropping the 16a values as these aren't true measurements but a result of reshaping the round 1 data.
```

## Create new variables

### Field variables

I originally made these new outcomes for just the round 1 data but I really want to have common outputs for plots by seasons that I can then turn into longitudinal outcomes. 

```{r}
fieldDat$dim <- fieldDat$field_length * fieldDat$field_width
fieldDat$are <- fieldDat$dim/100


inputVars <- names(fieldDat)[grep("fert_|quality_compost|type_compost|which_crop|which_maize", names(fieldDat))]

fieldDat[,inputVars] <- sapply(fieldDat[, inputVars], tolower)

# input quanitites
fieldDat$fert_kg_urea1 <- ifelse(fieldDat$fert_type1=="urea", fieldDat$fert_kg1, NA)
fieldDat$fert_kg_urea2 <- ifelse(fieldDat$fert_type2=="urea", fieldDat$fert_kg2, NA)
fieldDat$fert_total_urea <- apply(fieldDat[, grep("(urea.)", names(fieldDat))], 1, function(x){
  sum(as.numeric(x), na.rm=T)})



fieldDat$fert_kg_dap1 <- ifelse(fieldDat$fert_type1=="dap", fieldDat$fert_kg1, NA)
fieldDat$fert_kg_dap2 <- ifelse(fieldDat$fert_type2=="dap", fieldDat$fert_kg2, NA)
fieldDat$fert_total_dap <- apply(fieldDat[, grep("(dap.)", names(fieldDat))], 1, function(x){
  sum(as.numeric(x), na.rm=T)})



fieldDat$fert_kg_17npk1 <- ifelse(fieldDat$fert_type1=="npk-17", fieldDat$fert_kg1, NA)
fieldDat$fert_kg_17npk2 <- ifelse(fieldDat$fert_type2=="npk-17", fieldDat$fert_kg2, NA)
fieldDat$fert_total_17npk <- apply(fieldDat[, grep("(17npk.)", names(fieldDat))], 1, function(x){
  sum(as.numeric(x), na.rm=T)})



fieldDat$fert_kg_22npk1 <- ifelse(fieldDat$fert_type1=="npk-22", fieldDat$fert_kg1, NA)
fieldDat$fert_kg_22npk2 <- ifelse(fieldDat$fert_type2=="npk-22", fieldDat$fert_kg2, NA)
fieldDat$fert_total_22npk <- apply(fieldDat[, grep("(22npk.)", names(fieldDat))], 1, function(x){
  sum(as.numeric(x), na.rm=T)})



fieldDat$fert_kg_2555npk1 <- ifelse(fieldDat$fert_type1=="npk2555", fieldDat$fert_kg1, NA)
fieldDat$fert_kg_2555npk2 <- ifelse(fieldDat$fert_type2=="npk2555", fieldDat$fert_kg2, NA)
fieldDat$fert_total_2555npk <- apply(fieldDat[, grep("(2555npk.)", names(fieldDat))], 1, function(x){
  sum(as.numeric(x), na.rm=T)})


#lime
fieldDat$lime_outside <- ifelse(fieldDat$d_lime=="lime_outside", fieldDat$kg_lime, NA)
fieldDat$lime_tubura <- ifelse(fieldDat$d_lime=="lime_tubura", fieldDat$kg_lime, NA)
fieldDat$lime_both <- ifelse(fieldDat$d_lime=="both_tubura_non_tubura", fieldDat$kg_lime, NA)
```

```{r}
inputVars <- names(fieldDat)[grep("field_length|field_width|dim|fert_kg_|fert_total_|lime_", names(fieldDat))]

fieldDat[,inputVars] <-sapply(fieldDat[,inputVars], as.numeric)


#urea
fieldDat$fert_kgare_urea1 <- fieldDat$fert_kg_urea1/fieldDat$are
fieldDat$fert_kgare_urea2 <- fieldDat$fert_kg_urea2/fieldDat$are
fieldDat$fert_kgare_urea_total <- fieldDat$fert_total_urea/fieldDat$are

#dap
fieldDat$fert_kgare_dap1 <- fieldDat$fert_kg_dap1/fieldDat$are
fieldDat$fert_kgare_dap2 <- fieldDat$fert_kg_dap2/fieldDat$are
fieldDat$fert_kgare_dap_total <- fieldDat$fert_total_dap/fieldDat$are

#npk17
fieldDat$fert_kgare_17npk1 <- fieldDat$fert_kg_17npk1/fieldDat$are
fieldDat$fert_kgare_17npk2 <- fieldDat$fert_kg_17npk2/fieldDat$are
fieldDat$fert_kgare_17npk_total <- fieldDat$fert_total_17npk/fieldDat$are

#npk22
fieldDat$fert_kgare_22npk1 <- fieldDat$fert_kg_22npk1/fieldDat$are
fieldDat$fert_kgare_22npk2 <- fieldDat$fert_kg_22npk2/fieldDat$are
fieldDat$fert_kgare_22npk_total <- fieldDat$fert_total_22npk/fieldDat$are

#2555 npk
fieldDat$fert_kgare_2555npk1 <- fieldDat$fert_kg_2555npk1/fieldDat$are
fieldDat$fert_kgare_2555npk2 <- fieldDat$fert_kg_2555npk2/fieldDat$are
fieldDat$fert_kgare_2555npk_total <- fieldDat$fert_total_2555npk/fieldDat$are
```

#### Visualize field variables

```{r}
fieldInputVars <- names(fieldDat)[grep("field_length|field_width|dim|fert_kgare_", names(fieldDat))]


for(i in 1:length(fieldInputVars)){
    base <- ggplot(fieldDat, aes(x=fieldDat[,fieldInputVars[i]])) + labs(x = fieldInputVars[i], title=fieldInputVars[i])
    temp1 <- base + geom_density()
    temp2 <- base + geom_histogram()
    #temp2 <- boxplot(r[,numVars[i]],main=paste0("Variable: ", numVars[i]))
    multiplot(temp1, temp2, cols = 2)
}
```

### Soil variables

`soilOut` is the master soil data to which common changes are made. All resulting soil outcomes are made using that. Soil outcomes are named `soilOut.outcome_name`

```{r}
soilOut <- soilDat %>% mutate(
  measure = ifelse(season=="15b", 1, 
                   ifelse(season=="16b", 2,NA))
) %>% arrange(measure) %>%
  as.data.frame()

soilOut.Mean <- soilOut %>%
  group_by(sample_id) %>%
  summarize_each(
    funs(mean(., na.rm=T)), -c(SSN, season, sample_id, measure, d_client)
  ) %>% 
  ungroup() %>% 
  as.data.frame() %>%
  rename_(.dots = setNames(names(.), gsub("X\\.|\\.", "", names(.))))

# find a way to fit this into piping
names(soilOut.Mean)[2:19] <- paste0(names(soilOut.Mean)[2:19], ".mean")

# 0s are when we have only one observation
soilOut.Diff <- soilOut %>%
  group_by(sample_id) %>%
  # summarise_each(
  #   funs(if_else(length(.)==2, diff(x), .)), -c(SSN, season, sample_id, measure)
  # ) %>% ungroup() %>% as.data.frame()
  mutate_each(
    funs(. - lag(., default=first(.))), -c(SSN, season, sample_id, measure, d_client)
  ) %>%
  filter(measure==2) %>%
  as.data.frame() %>%
  rename_(.dots = setNames(names(.), gsub("X\\.|\\.", "", names(.))))

# find a way to fit this into piping
names(soilOut.Diff)[1:18] <- paste0(names(soilOut.Diff)[1:18], ".diff")


# gather soil outcomes to merge back together
#soilTrans <- list(ls()[grep("soilOut.", ls())])

soilMerge <- merge(soilOut.Mean, soilOut.Diff,by="sample_id")

```

### Demographic variables

```{r}
fieldDat$season_16a <- ifelse(grepl("16a", fieldDat$n_tubura_season), 1, 0)
fieldDat$season_16b <- ifelse(grepl("16b", fieldDat$n_tubura_season), 1, 0)
fieldDat$season_17a <- ifelse(grepl("17a", fieldDat$n_tubura_season), 1, 0)
fieldDat$notClient3Seasons <- ifelse(grepl("not_a_client", fieldDat$n_tubura_season), 1, 0)

```

Check field dimensions:

```{r}
ggplot(fieldDat, aes(x=field_width, y=field_length)) + 
  geom_point() +
  labs(title= "Field dimensions", x = "Width (m)", y= "Length (m)")
```

## Map of samples

```{r}
library(dismo)
if (!(exists("rwanda"))){
  # Only need to geocode once per session library(dismo)
  rwanda <- try(geocode("Rwanda"))
  # If the internet fails, use a local value 
  if (class(rwanda) == "try-error") {
    rwanda <- ""
    # arusha$longitude <- 36.68299
    # arusha$latitude <- -3.386925
  } 
}
```

See [here](http://rstudio-pubs-static.s3.amazonaws.com/208998_3592d3c6ac9a47ccbf3a3997ec2b68ec.html) for more on using markerClusterOptions in leaflet.

In the map below, the larger green circles are Tubura farmers and the smaller blue circles are control farmers. **The number of observations will appear larger on the map because it's plot level instead of farmer level.**

```{r leaflet, fig.width=9, fig.height=7}
e <- rs[!is.na(rs$lon),]
ss <- SpatialPointsDataFrame(coords = e[, c("lon", "lat")], data=e)

pal <- colorNumeric(c("navy", "green"), domain=unique(ss$client))
map <- leaflet() %>% addTiles() %>%
  setView(lng=rwanda$longitude, lat=rwanda$latitude, zoom=8) %>%
  addCircleMarkers(lng=ss$lon, lat=ss$lat, 
                   radius= ifelse(ss$client==1, 10,6),
                   color = pal(ss$client),
clusterOptions = markerClusterOptions(disableClusteringAtZoom=13, spiderfyOnMaxZoom=FALSE))

map
```

## Lessons for Nathaniel

Here are the key pieces of feedback for the next survey round:

* Variable naming convention - quite a bit of work had to be done to work with the data. Any plot specific variable should be named with _(year)(season) at the end. This will make it easy to reshape those variables into plot level variables.
* Check variables - some of the input variables are quite large. Is it possible to have CC automatically calculate quantities in a per are rate and signal the enumerator if the values seem high? Better field estimates should help with this but that sort of check would be a good reality check in the field.
* Soil texturing - how long did this take? I think we can have this done in the lab
* Seed types - not many farmers responded to the seed type question. Do we have a reason why from either farmers or enumerators? 
* NAs - so many NAs in the data! Why?
* Timing for upcoming survey
* **Commcare**: Please ensure that the variable labels are in the right language box. The export I'm getting directly from Commcare is a mix of English and Kinyarwanda names. I assume that's because the labels were not in the right boxes.



Analysis TODO:
* feature creation (in process)
* matching (talk to Maya)
  + 
* following previous template (look back)
  + 

For next week:
* talk with Maya about matching longitudinally
* soil graphs

# Analysis

Same as the baseline analysis but with two seasons of data

TODO: confirm that `d_client` is reflecting the right status as a farmer in the data. Is it baseline? Is it round 1? Is it a combo of the two?

## Initial soil graphs

These graphs are a peek at how soil parameter averages and differences look between treatment and control farmers. **This is a preliminary rough look**. Next steps include:

* Confirming client assignment and clarifying status
* Additional cleaning of soil variables
* Matching of clients to derive a more causal look at client effects on soil parameters.

```{r}
library(tidyr)
soilGraph <- soilMerge %>%
  gather(variable, value, -c(SSN, sample_id, measure, season, d_client)) %>%
  separate(variable, c("soilChar", "type"), sep="\\.")

for(i in 1:length(unique(soilGraph$soilChar))){
  for(j in 1:length(unique(soilGraph$type))){
    print(
     ggplot(soilGraph[soilGraph$soilChar==unique(soilGraph$soilChar)[i] & soilGraph$type==unique(soilGraph$type)[j],], aes(x = d_client, y=value)) + 
       geom_boxplot() + 
       labs(title = paste("NON-MATCHED PRELIM -", unique(soilGraph$soilChar)[i], unique(soilGraph$type)[j], sep=" "), x = "Treatment v. Control", y=unique(soilGraph$soilChar)[i])
    )
  }
}


```


## Matching

## Demographic summary

## Soil summary

## Longitudinal soil summary

For attributes, baseline attribute and round 1 value >> what's the trend?

## Yield Paired

How do soil attributes predict yields (climbing beans) >> can we understand yield as functions of carbon, pH, etc. Are the curves as we might expect?

### Yield data

The variable names from Commcare are in Kinyarwanda and a bit of a mess. I'm going to try to use the names from the Commcare form export. Or is there a way to get this information from Commcare? Surely there must be. 

```{r}
bean <- getFormData("oafrwanda", "M&E", "16B ALL Isarura (Harvest)", forceUpdate = forceUpdateAll)
write.csv(bean, file="rawCcYpData.csv", row.names=F)

yieldNames <- read.table(unz("2016B Harvest2017-06-08.zip", "Forms.csv"), nrows=10, header=T, quote="\"", sep=",") # only first 10 rows

# print variable names together
write.csv(data.frame(names(bean)[1:100], names(yieldNames)[1:100]), file="matchYieldNames.csv")

# get names from cc
# appName <- "M&E"
# formName <- "16B ALL Isarura (Harvest)"
# moduleIdx=NA
appData <- getAppStructure("oafrwanda")

enNames <- getFormFromApp(appData, "M&E", "16B ALL Isarura (Harvest)")$values

# leads to duplicates
onlyVarName <- strsplit(enNames, "/", fixed=F)

newNames <- do.call(rbind, lapply(onlyVarName, function(x){
  return(x[[length(x)]])
}))

names(bean)[10:length(names(bean))] <- newNames

#names(bean)[duplicated(names(bean))]

# update intercrop names so that they're unique >> manual cleaning
names(bean)[61:70] <- paste("plants_box1", names(bean)[61:70], sep="_")
names(bean)[82:91] <- paste("plants_box2", names(bean)[82:91], sep="_")

names(bean)[170] <- paste0("climbing_beans_", names(bean)[170])
names(bean)[177] <- paste0("bush_beans_", names(bean)[177])

names(bean)[171] <- paste0("climbing_beans_", names(bean)[171])
names(bean)[178] <- paste0("bush_beans_", names(bean)[178])

names(bean)[173] <- paste0("climbing_beans_", names(bean)[173])
names(bean)[180] <- paste0("bush_beans_", names(bean)[180])

names(bean)[174] <- paste0("climbing_beans_", names(bean)[174])
names(bean)[181] <- paste0("bush_beans_", names(bean)[181])

names(bean)[211] <- paste0("bush_beans", names(bean)[211])
names(bean)[221] <- paste0("maize_", names(bean)[221])
```

The best version of English names don't come from the data labels. They come from another portion of the output. I've extracted it here but a key point of feedback for Nathaniel will be to make certain that going forward variable labels are in the right places.

### Match soil and yield

It's probably safe to assume that if there isn't a soil code the data can be dropped. It's not clear how to match the yield data to the soil data. There might be a way to use the client id from the SHS data but I also don't know if that maps to the M&E data. I could try it if Nathaniel doesn't have a suggestion.

```{r}
#names(bean)[grep("soil",names(bean))]
#names(bean)[grep("id",names(bean))]
#table(bean$soil_code, useNA = 'ifany')
pairedSoilDir <- normalizePath(file.path("..", "..", "OAF Soil Lab Folder", "Projects", "rw_shs_16b_paired_climbing", "4_predicted", "other_summaries"))
pairedSoil <- read.csv(file=paste(pairedSoilDir, "combined-predictions-including-bad-ones.csv", sep = "/"))

pSoilIdDir <- normalizePath(file.path("..", "..", "OAF Soil Lab Folder", "Projects", "rw_shs_16b_paired_climbing", "5_merged"))
pSoilIds <- read.csv(file=paste(pSoilIdDir, "database.csv", sep = "/"))
```

### Clean soil ids

Helpful links: [mutate_each](https://stackoverflow.com/questions/27027347/mutate-each-summarise-each-in-dplyr-how-do-i-select-certain-columns-and-give) and [var names to lower](https://stackoverflow.com/questions/29264028/dplyr-or-magrittr-tolower)

```{r}
psi <- pSoilIds %>% 
  setNames(tolower(names(.))) %>%
  mutate_each(funs(tolower), district, cell) %>%
  rename(ssn = lab.ssn) %>% 
  mutate(
    idDups = duplicated(id) | duplicated(.[nrow(.):1, "id"])[nrow(.):1],
    ssnDups = duplicated(ssn) | duplicated(.[nrow(.):1, "ssn"])[nrow(.):1]
  )

pairedSoil <- pairedSoil %>% 
  setNames(tolower(names(.)))

#table(psi$ssn %in% pairedSoil$ssn) # FALSE  TRUE  41   703 
#table(pairedSoil$ssn %in% psi$ssn) # FALSE  TRUE  27   703 

pairedSoil <- left_join(pairedSoil, psi, by="ssn") # keeps all paired soil values, no duplicated ids
```

And now check how many soil ids are duplicated in the bean data. Is there any hope of untangling which ones are suppoed to be which based on the info provided in the soil data?

```{r}
beanCheck <- bean %>% 
  filter(!is.na(soil_code)) %>%
  mutate(
    idDups = duplicated(soil_code) | duplicated(.[nrow(.):1, "soil_code"])[nrow(.):1]
  )



beanCheck %>% 
  filter(idDups==TRUE) %>%
  arrange(soil_code) %>%
  dplyr::select(district, cell, soil_code)

```

And let's compare this to the ids in the soil data to see if we can find matches. If I can, I'll need to make a new unique id to match them.

```{r}
#vector of duplicated ids in the bean data
idComps <- unique(beanCheck$soil_code[beanCheck$idDups==TRUE])

pairedSoil %>% 
  filter(id %in% idComps) %>%
  arrange(id) %>%
  dplyr::select(district, cell, id)
```

Visually it doesn't seem that there are easy matches to be made. We obviously don't have any -88s or 0s in the id data. 

* `24764` Gitega g doesn't exist. 
* `44337` There are two murambi and we have no further distinguishing info.
* `183004` the name is entirely different.
* `1326301` kibyagira seems to be the best match!
* `9050401` the names are the same.
* `14160102` the names are the same.

Fix the one duplicate we can, drop the others and merge the yield data with the soil data. 
TODO - still waiting on Nathaniel for guidance on how to calculate climbing bean yield. I can take a look at this and see if I can guess.

TODO - follow up with Nathaniel about the soil ids not matching.

```{r}
bean <- bean[-which(bean$soil_code==1326301 & bean$cell=="Gahira"),]
```

```{r}
py <- bean %>% 
  filter(!is.na(soil_code)) %>%
  mutate(
    idDups = duplicated(soil_code) | duplicated(.[nrow(.):1, "soil_code"])[nrow(.):1]
  ) %>% 
  filter(idDups==FALSE) %>%
  rename(ns = id, # change the bean id to something else,  nonsense
         id = soil_code) 
```

We lose `r dim(beanCheck)[1] - dim(py)[1]` obs to duplicated or useless ids.

```{r}
loss <- table(py$id %in% pairedSoil$id)[[1]]
#py$id[!py$id %in% pairedSoil$id]
#table(pairedSoil$id %in% py$id)
```

We then lose `r loss` to not having matches. We're not getting good value for our money here.

```{r}
toJoin <- names(pairedSoil)[c(2:22,25)]

py <- py %>%
  inner_join(., pairedSoil[,toJoin], by="id")

```

### Clean and construct vars

I'm going to take a quick guess at how kg/m2 and t/ha yield calculations were made so that I can set up the analyses I want. I'm first incorporating chagnes to the data Alex Villec did in his .do file. See `cleans_harvest_16b.do` starting on line 85.

```{r}
py$box_length1 <- ifelse(py$d_box_lenght1!=7 & py$d_box_lenght1!=3, 5, py$d_box_lenght1)
py$box_width1 <- ifelse(py$d_box_width1!=7 & py$d_box_width1!=3, 5, py$d_box_width1)

py$box_length2 <- ifelse(py$d_box_length2!=7 & py$d_box_length2!=3, 5, py$d_box_length2)
py$box_width2 <- ifelse(py$d_box_width2!=7 & py$d_box_width2!=3, 5, py$d_box_width2)

```


```{r}
calculateYield <- function(bagA, bagB, lenA, lenB, widthA, widthB, df) {
  
  #convert to numeric
  df[,c(bagA, bagB, lenA, lenB, widthA, widthB)] <- sapply(df[,c(bagA, bagB, lenA, lenB, widthA, widthB)], function(x){
    as.numeric(as.character(x))
  })
  
  # calculate box areas
  df$boxAreaA <- df[,lenA] * df[,widthA]
  df$boxAreaB <- df[,lenB] * df[,widthB]

  df$yieldA <- df[,bagA] / df$boxAreaA
  df$yieldB <- df[,bagB] / df$boxAreaB

  df$yieldProbsA <- is.na(df$yieldA) | is.infinite(df$yieldA)
  df$yieldProbsB <- is.na(df$yieldB) | is.infinite(df$yieldB)

  df$yield <- (df[,bagA] + df[,bagB]) / (df$boxAreaA + df$boxAreaB)
  
  df$yield[!df$yieldProbsA & df$yieldProbsB] <- 
    df$yieldA[!df$yieldProbsA & df$yieldProbsB]
  df$yield[!df$yieldProbsB & df$yieldProbsA] <- 
    df$yieldB[!df$yieldProbsB & df$yieldProbsA]
  return(df)
}

py <- calculateYield("box_kg1", "box_kg2", "box_length1", "box_length2", "box_width1", "box_width2", py)

respVar <- c(names(py)[which(names(py)=="ph"): which(names(py)=="x.total.nitrogen")], "yield")

yr <- py[,names(py) %in% respVar]
yr$tha <- yr$yield * 10

soilVars <- names(yr)[which(names(yr)=="ph"):which(names(yr)=="x.total.nitrogen")]
```

```{r}

soilYTab <- do.call(plyr::rbind.fill, lapply(soilVars, function(y){
  #print(y)
  res = iqr.check(yr, y)
  #print(dim(res))
  out = data.frame(var=rbind(y, paste(y, ".iqr", sep="")), res)
  return(out)
}))

soilYTab[,2:length(soilYTab)] <- sapply(soilYTab[,2:length(soilYTab)], function(x){round(x,2)})
```

The outlier table summarizes the numeric variables with and without IQR outliers to show how the data changes based on this filter.

```{r}
knitr::kable(soilYTab, row.names = F, digits = 3, format = 'html')
```

Impose sensible constraints on soil variables

**Ask Patrick and Step what those might be**. I've removed IQR outliers but that's probably a bit too crude. 

```{r}
mat <- sapply(yr[,soilVars], function(x){
  q1 = summary(x)[[2]]
  q3 = summary(x)[[5]] 
  iqr = q3-q1
  check  = ifelse(x < (q1 - (1.5*iqr)) | x > (q3 + (1.5*iqr)), NA,x)
  return(check)
})

mat <- as.data.frame(mat)
names(mat) <-paste0(soilVars, ".iqr")
yr <- cbind(yr, mat)

soilIqr <- names(yr)[which(names(yr)=="ph.iqr"):which(names(yr)=="x.total.nitrogen.iqr")]

```


### Yield response curves

[Link to the diagPlot](https://rpubs.com/therimalaya/43190) and the [interpretation of linear diagnostics](http://strata.uga.edu/8370/rtips/regressionPlots.html) and guidance on [GridExtra](https://cran.r-project.org/web/packages/gridExtra/vignettes/tableGrob.html)
```{r}
diagPlot<-function(model){
  
    p1 <- ggplot(model, aes(.fitted, .resid)) + geom_point()
    p1 <- p1 + stat_smooth(method="loess") + geom_hline(yintercept=0, col="red", linetype="dashed")
    p1 <- p1 + xlab("Fitted values")+ylab("Residuals")
    p1 <- p1 + ggtitle("Residual vs Fitted Plot")+theme_bw()
    
    
    #p2Mod <- fortify(model)
    p2 <- ggplot() + geom_qq(data=model, aes(sample=.stdresid))
    p2<-p2+geom_abline()
    p2<-p2+ggtitle("Normal Q-Q")+theme_bw()
    
    p3<-ggplot(model, aes(.fitted, sqrt(abs(.stdresid))))+geom_point(na.rm=TRUE)
    p3<-p3+stat_smooth(method="loess", na.rm = TRUE)+xlab("Fitted Value")
    p3<-p3+ylab(expression(sqrt("|Standardized residuals|")))
    p3<-p3+ggtitle("Scale-Location")+theme_bw()
    
    # p4<-ggplot(model, aes(seq_along(.cooksd), .cooksd))+geom_bar(stat="identity", position="identity")
    # p4<-p4+xlab("Obs. Number")+ylab("Cook's distance")
    # p4<-p4+ggtitle("Cook's distance")+theme_bw()
    
    p5<-ggplot(model, aes(.hat, .stdresid))+geom_point(aes(size=.cooksd), na.rm=TRUE)
    p5<-p5+stat_smooth(method="loess", na.rm=TRUE)
    p5<-p5+xlab("Leverage")+ylab("Standardized Residuals")
    p5<-p5+ggtitle("Residual vs Leverage Plot")
    p5<-p5+scale_size_continuous("Cook's Distance", range=c(1,5))
    p5<-p5+theme_bw()+theme(legend.position="bottom")
    
    # p6<-ggplot(model, aes(.hat, .cooksd))+geom_point(na.rm=TRUE)+stat_smooth(method="loess", na.rm=TRUE)
    # p6<-p6+xlab("Leverage hii")+ylab("Cook's Distance")
    # p6<-p6+ggtitle("Cook's dist vs Leverage hii/(1-hii)")
    # p6<-p6+geom_abline(slope=seq(0,3,0.5), color="gray", linetype="dashed")
    # p6<-p6+theme_bw()
    
    return(list(rvfPlot=p1, 
                qqPlot=p2, 
                sclLocPlot=p3, 
                #cdPlot=p4, 
                rvlevPlot=p5
                #cvlPlot=p6
                ))
}
```

```{r}
plm <- function(x) { # x is a model
    require(gridExtra)
        # generate raw tables of useful information
        cp <- data.frame(coef(summary(x))) # coefficient and p-values
        ci <- data.frame(confint(x)) # 95% confidence intervals
        
        # strip out and format just what we need from cp into another table
        names(cp) <- c("Coefficient", "Std.Error", "T", "P")
        tab <- cp[, c("Coefficient", "P")]
        
        tab$Coefficient <- signif(tab$Coefficient, digits = 2)
        tab$P <- ifelse(tab$P < 0.001, paste("<0.001", "***"),
            ifelse(tab$P < 0.01 & tab$P >= 0.001, 
                paste(signif(tab$P, digits = 2), "**"), 
            ifelse(tab$P < 0.05 & tab$P >= 0.01, 
                paste(signif(tab$P, digits = 2), "*"),
            ifelse(tab$P < 0.1 & tab$P >= 0.05, 
                paste(signif(tab$P, digits = 2), "."), 
            round(tab$P, digits = 2)))))
        
        # add prettified confidence intervals to tab
        ci$X2.5.. <- signif(ci$X2.5.., digits = 2)
        ci$X97.5.. <- signif(ci$X97.5.., digits = 2)
        tab$CI <- paste(ci$X2.5.., ci$X97.5.., sep = " to ")
        
        # rearrange and rename tab
        tab <- tab[, c("Coefficient", "CI", "P")]
        names(tab) <- c("Coefficient", "95% Confidence Interval", "P-Value")
        
        # remove the district controls, which are always the last 
        tab = tab[!grepl("district", row.names(tab)), ]
        
        tt = ttheme_default(colhead=list(fg_params = list(parse=TRUE)))
        tabOut = tableGrob(tab, theme=tt,
                           rows=names(x$coefficients))  
        
        
        # make the plot and table
        do.call(grid.arrange, diagPlot(x))
        grid.arrange(tabOut)
        
        # grid.arrange(
        #  list(do.call(grid.arrange,diagPlot(x)),
        #       tabOut),
        #  nrow=2,
        #  top="Model Diagnostics"
        #  )
        #making the graphics go together
        
        
        # output = grid.arrange(plotOut,
        #                       tabOut, 
        #                       as.table=TRUE)
        
        #return(output)
    
    #return(do.call(cbind, tmp))
    #return(tmp)
}

```

#### Individual soil models

I'm not entirely certain how to best model yield as a function of soil properties. I'm going to run a handful of models and give some initial caveats. These model diagnositics still need to be incorporated into model use and interpretation. 

##### Full values - individual

**Obligatory disclaimer**: These summaries are intended to show if the regression models are reliable. We know already that the regressions are unable to be directly interpreted. We should try to find more established linear yield models to give our approach a firmer foundation in the literature.

```{r}
invisible(lapply(soilVars, function(x){
  #print(paste0("Soil variable: ", x))
  plm(lm(tha ~ yr[,x], data=yr))
  
}))
```

##### IQR values - individual
```{r}
invisible(lapply(soilIqr, function(x){
  #print(paste0("Soil variable: ", x))
  plm(lm(tha ~ yr[,x], data=yr))
  
}))
```


#### Individual soil curves

**Obligatory disclaimer**: These curves are simplistic and can't be taken at face value. We only have soil chemistry data and it's noisy soil chemistry data. We should follow more established yield models to understand the contribution of individual featuers toward yield response.

```{r}

respCurve <- function(dat, yVar, xVar, yLab, xLab, gTitle){
  lineSmooth <- ggplot(dat) + 
	  stat_smooth(aes(x = dat[,xVar], y=dat[,yVar]), se=FALSE) + 
	  theme_bw() + 
    labs(x = xLab, y = yLab, title=paste0("Basic response curve: ", gTitle))
  
  dotGraph <- ggplot(dat) + 
	  geom_point(aes(x = dat[,xVar], y=dat[,yVar]), se=FALSE) + 
	  theme_bw() + 
    labs(x = xLab, y = yLab, title=paste0("Scatter plot: ", gTitle))
  
  multiplot(lineSmooth, dotGraph)
}

```

##### Full values - curves

```{r}
# response curves
for(i in 1:length(soilVars)){
    respCurve(yr, "tha", soilVars[i],"Yield (t/ha)", soilVars[i], gTitle = soilVars[i])
  
}

```

##### IQR values - curves

```{r}
# response curves
for(i in 1:length(soilIqr)){
    respCurve(yr, "tha", soilIqr[i],"Yield (t/ha)", soilIqr[i], gTitle = soilIqr[i])

}

```

### Yield response tables

The concept here was to show the yield values at discrete x-axis values but this begets the question of how to calculate that y-value. The stylized lines above aren't sufficiently vetted to present them in a table. 

```{r}
yTable <- function(dat, yVars, xVar){
  
  fit = lm(yVar ~ xVars, dat)
  
}


```




# Summary

## Changes to the survey

# Appendix


