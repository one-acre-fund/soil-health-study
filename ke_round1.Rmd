---
title: "Kenya SHS Round 1 Data Check"
author: '[Matt Lowes](mailto:matt.lowes@oneacrefund.org)'
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_notebook:
    number_sections: yes
    code_folding: show
    theme: flatly
    toc: yes
    toc_depth: 6
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objective

The objective of this analysis is to identify potential data quality issues with in coming Kenya soil health round 1 study data so that the enumeration team can address issues in the field and improve overall quality. This file will also serve as (1) an initial cleaning file and (2) an attrition and balance check to inform when to wrap up enumeration.

# Key Takeaways

> 1) We have likely outliers in the numeric data. Follow up with enumerators about these values.
> 2) Many round 1 GPS points are not close to the baseline value. Follow up about plot continuity and GPS quality.

```{r, message=F}
source("../../../../../analyses/oaflib/commcareExport.R")
library(ggplot2)
library(readxl)
library(dplyr)
library(stringr)
library(reshape2)
library(readxl)
library(sp)
library(dismo)
library(XML)
library(leaflet)
library(ggmap)
suppressMessages(library(MASS))
```

# Data

## Import Data

I'm going to use the CommCare API created by [Robert On](robert.on@oneacrefund.org) to access the data directly from CommCare. This ensures that there are no changes to the data between the point of access and the point of cleaning and analysis

```{r}
forceUpdate = T
d <- getFormData("harvest", "Soil Sampling", "Soil Sampling 2017", forceUpdate)
# add in force update to get latest report
d <- as.data.frame(d)
```

# Analysis

The aim of the analysis is to: 

1. Check variables of interest and check for odd values that we might want to investigate further
2. Look at matches with the baseline and check GPS points
3. Look at matches with baseline and check for attrition and balance.

## Initial Cleaning 

The variable names are out of control. In order to make the variable names more user friendly, I went to [CommCare](www.commcarehq.com) and downloaded the form contents that includes the variable name. I then wrote in a more user friendly variable name in the file. I import and overwrite the variable names here. Perhaps there's a better way to do this but it's easier to log the changes in excel than to write out all the code. The reference files will be on hand in case I accidentally mislabel a variable. The variable names in the excel file start with variable 11 from CommCare. The first 10 CommCare variables are meta data which we will keep.

```{r, messages=F}
newName <- read_excel("var_names.xlsx", sheet=1)
```

```{r}
qTypes <- c("Multiple Choice", "Phone Number or Numeric ID", "Checkbox", "Text", "Decimal", "Image Capture", "Barcode Scan", "GPS", "Integer")

newName <- newName %>% dplyr::select(1:4
) %>% dplyr::filter(newName$Type %in% qTypes) %>% as.data.frame()
```

Keep the participation variable in `d` so we understand who agreed to participate. Make additional changes to `d` to prepare it to be combined with the meta information from CommCare.

```{r}
names(d)[23] <- "participation"
names(d)[names(d)=="X.form.id_base.Soil_Sample_Id"] <- "Soil_Sample_Id"
names(d)[names(d)=="X.form.id_2017new.Soil_Sample_Id"] <- "New_soil_sample_id"

# remove unnecessary vars from d
d <- d[,-which(grepl("^X", names(d)))]
#length(names(d)[11:length(names(d))])
#length(newName$new.var.name)

# subtract two because the soil_sample_id are out of order
#length(names(d)[11:(length(names(d))-2)]) == length(newName$new.var.name)
```
Merge in variable names to check that they match and then replace `names()`. I'm subtracting two because the `Soil_Sample_Id` variables appear out of order.

```{r}
names(d)[11:(length(d)-2)] <- newName$new.var.name

# drop vars with drop
d <- d[,-which(grepl("drop.", names(d)))]
```

Remove strange extra characters from the `d` data.

```{r}
charClean <- function(df){
  
  df <- as.data.frame(lapply(df, function(x){
  x = gsub("'", '', x)
  x = gsub("^b", '', x)
  x = ifelse(grepl("map object", x)==T, NA, x)
  return(x)
  }))
return(df)
}

d <- charClean(d)
```

Convert variables that are supposed to be numbers to numbers in `d`

```{r}
qNum <- c("Phone Number or Numeric ID", "Decimal", "Integer")
nums <- newName[newName$Type %in% qNum, "new.var.name"]
nums <- nums[-which(grepl("drop.", nums))]

toRemove <- c("phone", "oafid")
nums <- nums[!nums %in% toRemove]

# add in plot.size
nums <- c(nums, "plot.size")

d[, nums] <- as.data.frame(lapply(d[,nums], function(x){
  as.numeric(as.character(x))
}))
```

Replace factors as characaters in `d`.

```{r}
isFactor <- names(d)[sapply(d, is.factor)]
d[,isFactor] <- sapply(d[,isFactor], as.character)
```

Make new variables

```{r}
#convert acreage to m2 and then calculate fertilizer per acre
d$plot.m2 <- d$plot.size*4046

m2ToAcres <- function(input, meters) {
  res <- (input/meters)*4046
  return(res)
}

d$can.acre <- m2ToAcres(d$can.main, d$plot.m2)
d$dap.acre <- m2ToAcres(d$dap.main,d$plot.m2)
d$npk.acre <- m2ToAcres(d$npk.main,d$plot.m2)
d$urea.acre <- m2ToAcres(d$urea.main,d$plot.m2)
d$compost.acre <- m2ToAcres(d$compost.kgs,d$plot.m2)
d$seed.acre <- m2ToAcres(d$seed.kgs,d$plot.m2)

d$intercrop.seed.acre <- m2ToAcres(d$intercrop.seed.kgs, d$plot.m2)
d$intercrop.dap.acre <- m2ToAcres(d$dap.intercrop, d$plot.m2)
d$intercrop.can.acre <- m2ToAcres(d$can.intercrop, d$plot.m2)
d$intercrop.npk.acre <- m2ToAcres(d$npk.intercrop, d$plot.m2)
d$intercrop.urea.acre <- m2ToAcres(d$urea.intercrop, d$plot.m2)


nums <- c(nums, "can.acre", "dap.acre", "npk.acre", "urea.acre", "compost.acre", "seed.acre", "intercrop.seed.acre", "intercrop.dap.acre", 
"intercrop.can.acre", "intercrop.npk.acre", "intercrop.urea.acre")


```

Yield calculation - first let's confirm that when the `yield.unit` is NA that there isn't a yield value.

```{r}
table(d$yield[is.na(d$yield.unit)], useNA = 'ifany')
```

**Charles**: I assume a 0 yield is a true value. Or should it be NA? The code below converts all yield values associated with an NA unit to NA when in fact there should be some 0s.

```{r}
d$yield.kg <- ifelse(d$yield.unit=="100kg", d$yield*100, 
              ifelse(d$yield.unit=="50kg", d$yield*50,
              ifelse(d$yield.unit=="90kg", d$yield*90,
              ifelse(d$yield.unit=="GG", d$yield*2, NA))))
nums <- c(nums, "yield.kg")
```


## Outlier check

Check all numeric variables for outliers. First, it's probably safe to replace all -99s with NA

```{r}
d[d==-99] <- NA
```

```{r}
iqr.check <- function(x) { 
  q1 = summary(d[,x])[[2]]
  q3 = summary(d[,x])[[5]] 
  iqr = q3-q1
  mark  = ifelse(d[,x] < (q1 - (1.5*iqr)) | d[,x] > (q3 + (1.5*iqr)), 1,0)
  tab = rbind(
    summary(d[,x]),
    summary(d[mark==0, x])
  )
  return(tab)
}


iqrTab <- do.call(plyr::rbind.fill, lapply(nums, function(y){
  #print(y)
  res = iqr.check(y)
  #print(dim(res))
  out = data.frame(var=rbind(y, paste(y, ".iqr", sep="")), res)
  return(out)
}))

iqrTab[,2:8] <- sapply(iqrTab[,2:8], function(x){round(x,1)})
```

### Outlier table

The outlier table summarizes the numeric variables with and without IQR outliers to show how the data changes based on this filter.

```{r, results='asis'}
knitr::kable(iqrTab, row.names = F, digits = 0, format = 'html')
```

### Graphs

Here I graph all numeric values in the data to visually identify potential outliers. Future versions of this code can compare recently collected data against the full data distribution to better assess potentially suspect values.

```{r}
for(i in 1:length(nums)){
  print(
    ggplot(d, aes(x=d[,nums[i]])) + geom_density() +
      labs(x = nums[i])
  )
}
```

Quick test

```{r}
#d$can.main.test <- ifelse(d$can.main>100, NA, d$can.main)
```


### Print IQR outliers

**TODO**: add way to filter by date so we're only seeing the most recent outliers. This is useful for providing only the most up to date feedback for the enumeration team.

```{r}
# iqrOut <- function(x) { 
#   q1 = summary(d[,x])[[2]]
#   q3 = summary(d[,x])[[5]] 
#   iqr = q3-q1
#   mark  = ifelse(d[,x] < (q1 - (1.5*iqr)) | d[,x] > (q3 + (1.5*iqr)), 1,0)
#   # tab = rbind(
#   #   summary(d[,x]),
#   #   summary(d[mark==0, x])
#   # )
#   out = d[mark==1, c("district", "site", "metadata.username", "Soil_Sample_Id", "phone", x)]
#   out = melt(out, measure.vars = x)
#   out = out[!is.na(out$district),]
#   
#   return(out)
# }
# 
# #numsNotOaf <- nums[-grep("oafid", nums)]
# printIQR <- do.call(rbind, lapply(nums, function(y){
#   return(iqrOut(y))
# }))
# 
# printIQR <- printIQR[order(printIQR$metadata.username,printIQR$site, printIQR$Soil_Sample_Id),]
# 
# write.csv(printIQR, "ke_shs_r1 vars to check.csv", row.names = F)
```

### Outlier hypothesis testing

It will be the case that not all variables are normally distributed. If that's the case then IQR is not going to be helpful in flagging only outlying values. 

* Detect the type of distribution
* Feed this to a chi-squared test and have it check if each value is outside the assumed distribution
* Or transform the data according to the detection and use wilcox.test to test each value against the distribution

See [here](http://stackoverflow.com/questions/22433256/which-distribution-fits-data-better)

```{r}
detectOutlier <- function(dat, x){

# function checks distribution and if lognormal, coverts and outputs
# outliers from log normal distribution. have to remove 0s to check
  fits <- list(
 no = fitdistr(dat[,x][complete.cases(dat[,x])],"normal"),
 lo = fitdistr((dat[,x])[complete.cases(dat[,x]) & dat[,x]!=0 & dat[,x]>0],"log-normal")
 #ca = fitdistr(dat[,x][complete.cases(dat[,x])],"cauchy"),
 #we = fitdistr(dat[,x][complete.cases(dat[,x])], "weibull")
 )
# get the logliks for each model...
fitCheck <- sapply(fits, function(i) i$loglik)
bestFit <-names(which.max(fitCheck))

trans <- sapply(d[,x], function(y){
  ifelse(bestFit=="lo", log10(y+1), y)
})
  
  
  q1 = summary(trans)[[2]]
  q3 = summary(trans)[[5]] 
  iqr = q3-q1
  mark  = ifelse(trans < (q1 - (1.5*iqr)) | trans > (q3 + (1.5*iqr)), 1,0)
  # tab = rbind(
  #   summary(d[,x]),
  #   summary(d[mark==0, x])
  # )
  out = d[mark==1, c("district", "site", "metadata.username", "Soil_Sample_Id", "phone", x)]
  out = melt(out, measure.vars = x)
  out = out[!is.na(out$district),]
  
  return(out)

}

printIQR <- do.call(rbind, lapply(nums, function(y){
  #print(y)
  return(detectOutlier(d, y))
}))

printIQR <- printIQR[order(printIQR$metadata.username,printIQR$site, printIQR$Soil_Sample_Id),]
```

The initial output might still be showing values that the M&E team feels are reasonable values in reality despite being flagged as statistical outliers. I'm adding a function for [Charles](mailto:charles.ogechi@oneacrefund.org) and [Kalie](kalie.gold@oneacrefund.org) to say which values they don't want appearing in the summary.

I've included a couple examples of how we can filter the data through the code to arrive at a well documented list of values to investigate.

```{r}
modifyOutlierOutput <- function(dat, varname, val){
  dat = dplyr::filter(dat, variable!=varname & value!=val)
  return(dat)
}

printIQR <- modifyOutlierOutput(printIQR, "goats", 0)
printIQR <- modifyOutlierOutput(printIQR, "pigs", 1)
printIQR <- modifyOutlierOutput(printIQR, "slope", 0)

```

```{r}
write.csv(printIQR, "ke_shs_r1 vars to check update.csv", row.names = F)
```

### Print IQR outliers by enumerator

```{r}
for(i in 1:length(unique(printIQR$metadata.username))){
  outSheet <- printIQR[printIQR$metadata.username==unique(printIQR$metadata.username)[i], ]
  write.csv(outSheet, file=paste(getwd(), "enum_summaries",
          paste("ke_shs_r1 vars to check ", unique(printIQR$metadata.username)[i], ".csv", sep = ""), sep = "/"))
  
}
```

### Import feedback

Here I take the follow up responses from enumerators and update the values in the the full data
```{r}
updateDir <- paste(getwd(), "enum_summaries", sep = "/")

# files <- list.files(updateDir, pattern = ".csv")
# for(i in 1:length(files)){
#   temp <- read.csv(file=paste(updateDir, files[i], sep = "/"))
#   temp <- temp[!temp$update=="same", ]
#   temp$update <- as.numeric(as.character(temp$update))
#   temp$variable <- as.character(temp$variable)
#   
#   for(j in 1:nrow(temp)){
#     d[,temp[j,"variable"]][ d[,"oafid"]==temp[j,"oafid"] & d[,temp[j, "variable"]]==temp[j,"value"]] <- temp[j, "update"]
#   }
#   
# }

```

## GPS check

A couple notes:

* Generate 2017 sample GPS
* I'm bringing the data from the `ke_baseline.Rmd` file where the data was cleaned. 

```{r}
d <- cbind(d, str_split_fixed(d$gps, " ", n=4))
names(d)[names(d)=="1" |names(d)== "2" | names(d)== "3" | names(d)== "4"] <- c("lat", "lon", "alt", "precision")
d[,c("lat", "lon", "alt", "precision")] <- sapply(
  d[,c("lat", "lon", "alt", "precision")],                                          function(x){as.numeric(as.character(x))})

# make round 1 soil_sample_id lowercase
d$Soil_Sample_Id <- tolower(d$Soil_Sample_Id)
trim <- function (x) {
  gsub("^\\s+|\\s+$", "", x)
  }
d$Soil_Sample_Id <- trim(d$Soil_Sample_Id)
d$Soil_Sample_Id <- gsub(" ", "", d$Soil_Sample_Id)
```

```{r}
ba <- read.csv(file="../ke baseline/data/shs ke baseline.csv", stringsAsFactors = F)
gpsCol <- c("lat", "lon", "alt", "precision")
for(i in 1:length(gpsCol)){
  names(ba)[names(ba)==gpsCol[i]] <- paste(gpsCol[i], ".ba", sep = "")
}
```

Merge the baseline and round 1 data using `Soil_Sample_Id`.

```{r}
table(ba$Soil_Sample_Id %in% d$Soil_Sample_Id)
```

We're getting `r table(ba$Soil_Sample_Id %in% d$Soil_Sample_Id)[[2]]` out of `r dim(d)[1]` matches. Let's look at the observations in round 1 that are currently not matching the baseline.

```{r}
d$Soil_Sample_Id[!d$Soil_Sample_Id %in% ba$Soil_Sample_Id]
```

**Question for Charles**: Why would we have NA or empty `Soil_Sample_Id` in `d`?

### Merge data

This merge will bring the baseline GPS into `d` and compare the values.

```{r}
gpsCheck <- inner_join(d, ba[,c("Soil_Sample_Id", "lat.ba", "lon.ba", "alt.ba", "precision.ba")], by="Soil_Sample_Id")
```

### Compare GPS points

The graph below is an example of what I'm trying to account for. All new GPS points should match the baseline GPS points within a certain margin of error. Most points align well however there are clearly identifiable points that do not match.

#### Latitute Comparison

```{r, fig.height=6, fig.width=8}
ggplot(gpsCheck, aes(x=lat.ba, y=lat)) + geom_point()
```

#### Longitude Comparison

```{r,fig.height=6, fig.width=8}
ggplot(gpsCheck, aes(x=lon.ba, y=lon)) + geom_point()
```

### Calculate distance

I'm calculating the distance between the baseline GPS points and the round 1 GPS points to see how far off we were. See [here](http://stackoverflow.com/questions/31668163/geographic-distance-between-2-lists-of-lat-lon-coordinates) for reference.

```{r}
library(sp)
gpsCheck$dist.km <- sapply(1:nrow(gpsCheck),function(i)
                spDistsN1(as.matrix(gpsCheck[i,c("lon", "lat")]),as.matrix(gpsCheck[i,c("lon.ba", "lat.ba")]),longlat=T))

gpsCheck$dist.m <- gpsCheck$dist.km*1000
```

```{r}
gpsTab <- gpsCheck[gpsCheck$dist.m>100, c("metadata.username", "district", "site", "dist.m")]
gpsTab <- gpsTab[order(gpsTab$dist.m, decreasing = T),]
```

We have `r dim(gpsTab)[1]` round 1 GPS points that are 100m from the baseline point. 

### Distance Map

```{r}
mapPrep <- gpsCheck %>% top_n(n=20, wt=dist.m) 

#mapPrep[,c("Soil_Sample_Id","lat.ba", "lat", "lon.ba", "lon", "dist.m")]

distDat <- data.frame(id = mapPrep$Soil_Sample_Id,
                      lat = c(mapPrep$lat.ba, mapPrep$lat),
                      lon = c(mapPrep$lon.ba, mapPrep$lon))
distDat <- distDat[order(distDat$id),]
distDat$baseline <- c(1,0)
```

```{r, fig.height=6, fig.width=8}
e <- distDat[!is.na(distDat$lat),]
ss <- SpatialPointsDataFrame(coords = e[, c("lon", "lat")], data=e)

ke <- try(dismo::geocode("Kenya"))
pal <- colorNumeric(c("navy", "green"), domain=unique(ss$baseline))
map <- leaflet() %>% addTiles() %>%
setView(lng=ke$longitude, lat=ke$latitude, zoom=6) %>%
  addPolylines(data = ss, lng = ~lon, lat = ~lat, group = ~id) %>%
  addCircleMarkers(lng=ss$lon, lat=ss$lat, 
                   radius= ifelse(ss$baseline==1, 10,6),
                   color = pal(ss$baseline))

map
```

> Green circles are baseline values, navy circles are from the follow up. We can see that both at the baseline and at the follow up GPS were logged at the office.

```{r, results='asis'}
knitr::kable(gpsTab, col.names = c("Enumerator", "District", "Site", "Distance (m)"))
```

**Action Point**: I'm outputting these rows for Charles and team to follow up. We shouldn't be 70km off. Let me know if other information would be helpful for following up with these enumerators

```{r}
gpsOut <- gpsCheck[gpsCheck$dist.m>100,c("metadata.username", "district", "site", "dist.m", "Soil_Sample_Id")]
write.csv(gpsOut, file="gps_to_check.csv")
```


## Attrition and Balance

*Use merged data to understand who we haven't found yet and check for balance* We're currenly missing `r dim(ba)[1]-dim(d)[1]` observations
